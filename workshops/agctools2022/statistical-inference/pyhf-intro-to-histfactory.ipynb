{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to HistFactory Models\n",
    "\n",
    "## HEP-like?\n",
    "\n",
    "So what do we do as experimentalists in High Energy Physics? We have a gigantic detector that serves as a counting machine for physics. We smash particles and take pictures of the aftermath. Then we propose hypotheses for new physics that we want to test using this data. So we will have, at the very least:\n",
    "\n",
    "* signal\n",
    "* background (with some uncertainty)\n",
    "* some observations\n",
    "\n",
    "This is probably the simplest model one could use in HistFactory... so let's make it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pyhf\n",
    "from pyhf.contrib.viz import brazil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pyhf.simplemodels.uncorrelated_background(\n",
    "    signal=[5.0, 10.0], bkg=[50.0, 60.0], bkg_uncertainty=[5.0, 12.0]\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did we just make? This returns a [`pyhf.pdf.Model`](https://pyhf.readthedocs.io/en/v0.6.3/_generated/pyhf.pdf.Model.html#pyhf.pdf.Model) object. Let's check out the specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(model.spec, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the specification, we defined a single two-bin channel ('singlechannel') with two samples: a **signal sample** and a **background sample**. The signal sample has an _unconstrained normalization factor_ $\\mu$, the signal strength, while the background sample carries an _uncorrelated shape systematic_ controlled by parameters $\\gamma_{1}$ and $\\gamma_{2}$. The _background uncertainty_ for the bins is $10\\%$ and $20\\%$ respectively.\n",
    "\n",
    "These uncertainties are **absolute** (not relative!). We have 10% and 20% relative uncertainty for the bins in the background sample respectively.\n",
    "\n",
    "(*Note: we have a workspace defined with this simple model in `data/2-bin_1-channel.json` which we will look at later.*)\n",
    "\n",
    "Let's explore the model a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"  channels: {model.config.channels}\")\n",
    "print(f\"     nbins: {model.config.channel_nbins}\")\n",
    "print(f\"   samples: {model.config.samples}\")\n",
    "print(f\" modifiers: {model.config.modifiers}\")\n",
    "print(f\"parameters: {model.config.parameters}\")\n",
    "print(f\"  nauxdata: {model.config.nauxdata}\")\n",
    "print(f\"   auxdata: {model.config.auxdata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa, hold up! What's with the auxiliary data? Recall the HistFactory definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "p(n,a|\\theta) = \\underbrace{\\prod_{\\mathrm{channel}\\ c}\\prod_{\\mathrm{bin}\\ b} \\mathrm{Pois}(n_{cb} | \\lambda_{cb}(\\theta))}_{\\text{main}} \\underbrace{\\prod_{\\mathrm{constraint}\\ \\chi} p_\\chi(a_\\chi | \\chi)}_{\\text{auxiliary}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for\n",
    "\n",
    "$$\n",
    "\\lambda_{cb}(\\theta) = \\sum_{\\mathrm{sample}\\ s} \\lambda_{cbs}(\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the auxiliary data here is passed into a constraint function. There's two things going on here:\n",
    "\n",
    "1. The uncorrelated shapesys modifier is constrained by a Poisson.\n",
    "2. The auxiliary data is fully determined by the shapesys 'data' and the 'background' data.\n",
    "\n",
    "So if we were to explicitly write out the likelihood we were seeing symbolically, it would be:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p_\\text{main} \\cdot p_\\text{aux} &= \\text{Pois}(n | \\lambda) \\cdot p_\\text{aux}\\\\\n",
    "                                 &= \\text{Pois}(n | \\mu s + \\gamma b) \\cdot p_\\text{aux}\\\\\n",
    "                                 &= \\text{Pois}(n | \\mu s + \\gamma b) \\cdot \\text{Pois}(n_\\text{aux} = (b/\\delta b)^2 | \\mu = (b/\\delta b)^2 \\gamma )\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $n = \\{n_1, n_2\\}$ for a 2-bin model (we're being slightly fast and loose with our mathematical notation here), and similarly for $s$, $b$, and $\\gamma$.\n",
    "\n",
    "The 'shapesys' is defined in the [HistFactory paper](https://cds.cern.ch/record/1456844)... however, it can be a little hard to extract out the necessary information. We've provided a nice table of [Modifiers and Constraints](https://pyhf.readthedocs.io/en/v0.6.3/intro.html#id24) in the introduction of our pyhf documentation to use as reference.\n",
    "\n",
    "![modifiers and constraints](https://github.com/pyhf/pyhf-tutorial/raw/main/book/assets/modifiers_and_constraints.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first row here which is our uncorrelated shape modifier. This is a multiplicative modifier denoted by $\\kappa$ per-bin (denoted by $\\gamma_b$). Notice that the input for the constraint term requires $\\sigma_b$ which is the relative uncertainty of that modifier. This is Poisson-constrained by $\\sigma_b^{-2}$. Let's quickly calculate \"by hand\" the auxiliary data to convince ourselves of what's going on here (remembering that the background uncertainties were 10% and 20% of the observed background counts):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.array([5.0, 12.0]) / np.array([50.0, 60.0])) ** -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is what we see from the `pyhf.pdf.Model` API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.auxdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Parameters\n",
    "\n",
    "What about the actual data for the model then?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "model.expected_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hrmm, that didn't work. It seems we need to specify the parameter values to get the data for the model at those parameter values. Well, we know the default for $\\mu=1$ and $\\gamma = 1$. Also recall that the uncorrelated shape allocates a parameter per bin, so our model has **3 parameters** with **2 modifiers**. So let's just try that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.expected_data([1.0, 1.0, 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns the expected data given the model parameters for the entire likelihood for the 2 bin model, the main model as well as the constraint (or auxiliary) model. We can also drop the auxdata to get the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.expected_data([1.0, 1.0, 1.0], include_auxdata=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, there are also methods separately for the actual data and the auxdata, both which take in all the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.expected_actualdata([1.0, 1.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.expected_auxdata([1.0, 1.0, 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Parameter Ordering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we know what these parameters correspond to? The way `pyhf` works under the hood, you can imagine an `n`-dimensional tensor that is purely bookkeeping all the different expected rates and modifications (read: systematics) involved that we can apply tensor algebra calculations to. This means that the ordering of the entries in each dimension is important, so we need to keep track of the order of the channels, the samples, the parameters. In `pyhf`, you can ask for the parameters or modifiers in a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.modifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, these might **not necessarily** correspond with the order of the parameters when we build this tensor internally, and we keep this order under a different variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.par_order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Parameter Multiplicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this clearly doesn't answer everything, because we were using three numbers assigned to two parameters! How does that work? Well, recall that the only channel in this model has 2 bins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.channel_nbins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and one of the parameters (the uncorrelated shape systematic) will be configured by $\\gamma_b$ &mdash; that is, one parameter for each bin of the sample in that channel. How does `pyhf` know that the parameter set is associated with two parameters? We can ask for information about the `paramset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.param_set(\"uncorr_bkguncrt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which we can see is constrained by a Poisson and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.param_set(\"uncorr_bkguncrt\").n_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Parameter Defaults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we know what we should set the parameters to? The nice thing is that this is up to you! The model will come with some suggested defaults for parameters..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.suggested_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and their bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.suggested_bounds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and if the parameter should be held constant in a fit or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.suggested_fixed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So armed with this knowledge, we could have also done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_pars = model.config.suggested_init()\n",
    "model.expected_actualdata(init_pars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the expected actual data from the model (signal + background), we could turn off the signal, corresponding with the normalization factor. We just need to know the index of the parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.poi_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then just change the parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkg_pars = init_pars.copy()\n",
    "bkg_pars[model.config.poi_index] = 0\n",
    "model.expected_actualdata(bkg_pars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Inference\n",
    "\n",
    "The core of statistical analysis is the statistical model. For inference, it's viewed as a function of the model parameters conditioned on the fixed observations.\n",
    "\n",
    "$$\n",
    "\\log L(\\theta | x) \\propto \\log p(x | \\theta)\n",
    "$$\n",
    "\n",
    "The value of the likelihood is a float. Let's try it for both the background-only model as well as the signal+background model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = [53.0, 65.0] + model.config.auxdata  # this is a common pattern!\n",
    "\n",
    "model.logpdf(pars=bkg_pars, data=observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.logpdf(pars=init_pars, data=observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not performing inference just yet. We're simply computing the 'logpdf' of the model specified by the parameters $\\theta$ against the provided data. To perform a fit, we use the [inference API](https://pyhf.readthedocs.io/en/v0.6.3/api.html#inference) via `pyhf.infer`.\n",
    "\n",
    "When fitting a model to data, we usually want to find the $\\hat{\\theta}$ which refers to the \"Maximum Likelihood Estimate\" of the model parameters. This is often referred to mathematically by\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_\\text{MLE} = \\text{argmax}_\\theta L(\\theta | x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform a unconstrained maximum likelihood fit on this model to the provided observations we just made up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyhf.infer.mle.fit(data=observations, pdf=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what can we say? With nominal signal `[5, 10]` and nominal background = `[50, 60]` model components, an observed count of `[53, 65]` suggests best fit values:\n",
    "* $\\hat{\\mu} \\approx 0.54$,\n",
    "* $\\hat{\\gamma} \\approx [1,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Hypothesis Testing\n",
    "\n",
    "Great, so we can fit. But that's not usually the interesting part, since we cannot make any statements about this. What we prefer to do is perform a hypothesis test to either:\n",
    "\n",
    "* Reject Standard Model hypothesis &mdash; a discovery fit.\n",
    "* Reject Beyond the Standard Model hypothesis &mdash; an exclusion fit.\n",
    "\n",
    "So we need to compute test statistics in order to evaluate the hypothesis. If we know the distribution of the test statistic under two different hypotheses, we can compute a [CLs](https://en.wikipedia.org/wiki/CLs_method_(particle_physics)) value. This is a modified pseudo-frequentist $p\\,$-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{CL}_\\text{s} = \\frac{\\text{CL}_\\text{s+b}}{\\text{CL}_\\text{b}} = \\frac{p_{\\text{s+b}}}{1-p_{\\text{b}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with\n",
    "\n",
    "$$\n",
    "\\text{CL}_\\text{s+b} = \\int_{t_\\text{obs}}^\\infty \\text{d}t\\ p_0(t | H_0) \\qquad\\qquad \\text{CL}_\\text{b} = \\int_{t_\\text{obs}}^\\infty \\text{d}t\\ p_1(t | H_1)\n",
    "$$\n",
    "\n",
    "This is done by splitting the model parameters up into two groups:\n",
    "\n",
    "* parameters of interest (POI &mdash; can have multiple!)\n",
    "* nuisance parameters (NP)\n",
    "\n",
    "Recall that additionally, all parameters are either constrained or unconstrained. Parameters of interest happen to always be unconstrained parameters.\n",
    "\n",
    "$$\n",
    "    f(\\boldsymbol{x}|\\phi) = f(\\boldsymbol{x}|\\overbrace{\\eta}^{\\llap{\\text{free}}},\\underbrace{\\chi}_{\\llap{\\text{constrained}}}) = f(\\boldsymbol{x}|\\overbrace{\\psi}^{\\rlap{\\text{parameters of interest}}},\\underbrace{\\theta}_{\\rlap{\\text{nuisance parameters}}})\n",
    "$$\n",
    "\n",
    "In the vast majority of cases, the test statistic is the profile likelihood ratio, or variations of it:\n",
    "\n",
    "$$\n",
    "t_\\psi(x) = \\frac{L(\\psi, \\hat{\\hat{\\theta}})}{L({\\hat{\\psi}, \\hat{\\theta}})}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "* $\\hat{\\hat{\\theta}}$ is the best fitted value of the nuisance parameters (for fixed POIs)\n",
    "* $\\hat{\\psi}$ and $\\hat{\\theta}$ are the best fitted values in a global fit\n",
    "\n",
    "So let's run a limit setting (exclusion) hypothesis test for\n",
    "\n",
    "* test hypothesis ($\\mu = 1$) &mdash; \"BSM physics process exists\"\n",
    "* alternate hypothesis ($\\mu = 0$) &mdash; \"Standard Model only physics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "    1.0,  # test hypothesis\n",
    "    [53.0, 65.0] + model.config.auxdata,\n",
    "    model,\n",
    "    test_stat=\"q\",\n",
    "    return_expected_set=True,\n",
    ")\n",
    "print(f\"      Observed CLs: {CLs_obs:.4f}\")\n",
    "for expected_value, n_sigma in zip(CLs_exp, np.arange(-2, 3)):\n",
    "    print(f\"Expected CLs({n_sigma:2d} σ): {expected_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nicely enough, `pyhf` is smart enough to let us know that, if for a model where $\\hat{\\psi} < 0$, we're not going to be able to catch it because our default bounds were bounding the parameter of interest from below at 0. For more information, see Equation 14 in [[1007.1727]](https://arxiv.org/abs/1007.1727)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.suggested_bounds()[model.config.poi_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should heed the warning and switch our test statistic in this case to a variation of it $q_\\mu \\to \\tilde{q}_\\mu$. We could also change the bounds from the default to allow $\\mu$ to float below zero. In this case, we're just going to use a more appropriate test statistic as we know $\\hat{\\mu} \\geq 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLs_obs, CLs_exp = pyhf.infer.hypotest(\n",
    "    1.0,  # test hypothesis\n",
    "    [53.0, 65.0] + model.config.auxdata,\n",
    "    model,\n",
    "    test_stat=\"qtilde\",\n",
    "    return_expected_set=True,\n",
    ")\n",
    "print(f\"      Observed CLs: {CLs_obs:.4f}\")\n",
    "for expected_value, n_sigma in zip(CLs_exp, np.arange(-2, 3)):\n",
    "    print(f\"Expected CLs({n_sigma:2d} σ): {expected_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Upper Limit\n",
    "\n",
    "To get upper limits, we just need to run multiple hypothesis tests for a lot of different null hypotheses of BSM with $\\mu \\in [0, \\ldots, 5.0]$ and then find the value of $\\mu$ for which the null hypothesis is rejected (a 95% $\\text{CL}_\\text{s}$). We can do all of this very easily just using the [`upperlimit` API](https://pyhf.readthedocs.io/en/v0.6.3/_generated/pyhf.infer.intervals.upperlimit.html), which also can calculate the upper limit by interpolating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "poi_values = np.linspace(0.1, 5, 50)\n",
    "obs_limit, exp_limits, (scan, results) = pyhf.infer.intervals.upperlimit(\n",
    "    observations, model, poi_values, level=0.05, return_results=True\n",
    ")\n",
    "print(f\"Upper limit (obs): μ = {obs_limit:.4f}\")\n",
    "print(f\"Upper limit (exp): μ = {exp_limits[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the standard \"Brazil band\" of the observed and expected $\\text{CL}_\\text{s}$ values using the `pyhf.contrib` module (which needs `pyhf[contrib]`):\n",
    "\n",
    "The horiztonal red line indicates the test size ($\\alpha=0.05$), whose intersection with the $\\text{CL}_\\text{s}$ lines visually represents the $(1-\\alpha)\\%$ CL limit on the POI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10.5, 7)\n",
    "ax.set_title(\"Hypothesis Tests\")\n",
    "\n",
    "artists = brazil.plot_results(poi_values, results, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if you wanted to do all of this \"by hand\" you still could pretty easily. The `pyhf.infer.intervals.upperlimit` API just makes it easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a hypothesis test scan across POIs\n",
    "results = [\n",
    "    pyhf.infer.hypotest(\n",
    "        poi_value,\n",
    "        observations,\n",
    "        model,\n",
    "        test_stat=\"qtilde\",\n",
    "        return_expected_set=True,\n",
    "    )\n",
    "    for poi_value in poi_values\n",
    "]\n",
    "\n",
    "# Calculate upper limit through interpolation\n",
    "observed = np.asarray([h[0] for h in results]).ravel()\n",
    "expected = np.asarray([h[1][2] for h in results]).ravel()\n",
    "print(f\"Upper limit (obs): μ = {np.interp(0.05, observed[::-1], poi_values[::-1]):.4f}\")\n",
    "print(f\"Upper limit (exp): μ = {np.interp(0.05, expected[::-1], poi_values[::-1]):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
