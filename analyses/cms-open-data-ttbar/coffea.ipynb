{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c40fe12f",
   "metadata": {},
   "source": [
    "# CMS Open Data $t\\bar{t}$: from data delivery to statistical inference\n",
    "\n",
    "We are using [2015 CMS Open Data](https://cms.cern/news/first-cms-open-data-lhc-run-2-released) in this demonstration to showcase an analysis pipeline.\n",
    "It features data delivery and processing, histogram construction and visualization, as well as statistical inference.\n",
    "\n",
    "This notebook was developed in the context of the [IRIS-HEP AGC tools 2022 workshop](https://indico.cern.ch/e/agc-tools-2).\n",
    "This work was supported by the U.S. National Science Foundation (NSF) Cooperative Agreement OAC-1836650 (IRIS-HEP).\n",
    "\n",
    "This is a **technical demonstration**.\n",
    "We are including the relevant workflow aspects that physicists need in their work, but we are not focusing on making every piece of the demonstration physically meaningful.\n",
    "This concerns in particular systematic uncertainties: we capture the workflow, but the actual implementations are more complex in practice.\n",
    "If you are interested in the physics side of analyzing top pair production, check out the latest results from [ATLAS](https://twiki.cern.ch/twiki/bin/view/AtlasPublic/TopPublicResults) and [CMS](https://cms-results.web.cern.ch/cms-results/public-results/preliminary-results/)!\n",
    "If you would like to see more technical demonstrations, also check out an [ATLAS Open Data example](https://indico.cern.ch/event/1076231/contributions/4560405/) demonstrated previously.\n",
    "\n",
    "This notebook implements most of the analysis pipeline shown in the following picture, using the tools also mentioned there:\n",
    "![ecosystem visualization](utils/ecosystem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f59a138",
   "metadata": {},
   "source": [
    "### Data pipelines\n",
    "\n",
    "To be a bit more precise, we are going to be looking at three different data pipelines:\n",
    "![processing pipelines](utils/processing_pipelines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b92c4c",
   "metadata": {},
   "source": [
    "### Imports: setting up our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2130f252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "import logging\n",
    "\n",
    "import vector; vector.register_awkward()\n",
    "\n",
    "import awkward as ak\n",
    "import cabinetry\n",
    "from coffea import processor\n",
    "from coffea.processor import servicex\n",
    "from coffea.nanoevents import transforms\n",
    "from coffea.nanoevents.methods import base, vector\n",
    "from coffea.nanoevents.schemas.base import BaseSchema, zip_forms\n",
    "from func_adl import ObjectStream\n",
    "import hist\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import uproot\n",
    "\n",
    "import utils  # contains code for bookkeeping and cosmetics, as well as some boilerplate\n",
    "\n",
    "logging.getLogger(\"cabinetry\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f3c55c-3159-4120-9705-e9ffd194b822",
   "metadata": {},
   "source": [
    "### Configuration: number of files and data delivery path\n",
    "\n",
    "The number of files per sample set here determines the size of the dataset we are processing.\n",
    "There are 9 samples being used here, all part of the 2015 CMS Open Data release.\n",
    "They are pre-converted from miniAOD files into ntuple format, similar to nanoAODs.\n",
    "More details about the inputs can be found [here](https://github.com/iris-hep/analysis-grand-challenge/tree/main/datasets/cms-open-data-2015).\n",
    "\n",
    "The table below summarizes the amount of data processed depending on the `N_FILES_MAX_PER_SAMPLE` setting.\n",
    "\n",
    "| setting | number of files | total size |\n",
    "| --- | --- | --- |\n",
    "| `10` | 90 | 15.6 GB |\n",
    "| `100` | 850 | 150 GB |\n",
    "| `500` | 3545| 649 GB |\n",
    "| `1000` | 5864 | 1.05 TB |\n",
    "| `-1` | 22635 | 3.44 TB |\n",
    "\n",
    "The input files are all in the 100â€“200 MB range.\n",
    "\n",
    "Some files are also rucio-accessible (with ATLAS credentials):\n",
    "\n",
    "| dataset | number of files | total size |\n",
    "| --- | --- | --- |\n",
    "| `user.ivukotic:user.ivukotic.ttbar__nominal` | 7066 | 1.46 TB |\n",
    "| `user.ivukotic:user.ivukotic.ttbar__scaledown` | 902 | 209 GB |\n",
    "| `user.ivukotic:user.ivukotic.ttbar__scaleup` | 917 | 191 GB |\n",
    "| `user.ivukotic:user.ivukotic.ttbar__ME_var` | 438 | 103 GB |\n",
    "| `user.ivukotic:user.ivukotic.ttbar__PS_var` | 443 | 100 GB |\n",
    "| `user.ivukotic:user.ivukotic.single_top_s_chan__nominal` | 114 | 11 GB |\n",
    "| `user.ivukotic:user.ivukotic.single_top_t_chan__nominal` | 2506 | 392 GB |\n",
    "| `user.ivukotic:user.ivukotic.single_top_tW__nominal` | 50 | 9 GB |\n",
    "| `user.ivukotic:user.ivukotic.wjets__nominal` | 10199 | 1.13 TB |\n",
    "| total | 22635 | 3.61 TB |\n",
    "\n",
    "The difference in total file size is presumably due to the different storages, which report slightly different sizes.\n",
    "\n",
    "When setting the `PIPELINE` variable below to `\"servicex_databinder\"`, the `N_FILES_MAX_PER_SAMPLE` variable is ignored and all files are processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b547996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GLOBAL CONFIGURATION\n",
    "\n",
    "# input files per process, set to e.g. 10 (smaller number = faster)\n",
    "N_FILES_MAX_PER_SAMPLE = -1\n",
    "\n",
    "# pipeline to use:\n",
    "# - \"coffea\" for pure coffea setup\n",
    "# - \"servicex_processor\" for coffea with ServiceX processor\n",
    "# - \"servicex_databinder\" for downloading query output and subsequent standalone coffea\n",
    "PIPELINE = \"servicex_databinder\"\n",
    "\n",
    "# enable Dask (may not work yet in combination with ServiceX outside of coffea-casa)\n",
    "USE_DASK = True\n",
    "\n",
    "# ServiceX behavior: ignore cache with repeated queries\n",
    "SERVICEX_IGNORE_CACHE = True\n",
    "\n",
    "# analysis facility: set to \"coffea_casa\" for coffea-casa environments, \"EAF\" for FNAL, \"local\" for local setups\n",
    "AF = \"coffea_casa\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8674317-8ecd-402c-a1fe-d5de1074705e",
   "metadata": {},
   "source": [
    "### Defining our `coffea` Processor\n",
    "\n",
    "The processor includes a lot of the physics analysis details:\n",
    "- event filtering and the calculation of observables,\n",
    "- event weighting,\n",
    "- calculating systematic uncertainties at the event and object level,\n",
    "- filling all the information into histograms that get aggregated and ultimately returned to us by `coffea`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "790dc3c2-7311-4ad4-a177-4fd4a1553559",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processor_base = processor.ProcessorABC if (PIPELINE != \"servicex_processor\") else servicex.Analysis\n",
    "\n",
    "# functions creating systematic variations\n",
    "def flat_variation(ones):\n",
    "    # 2.5% weight variations\n",
    "    return (1.0 + np.array([0.025, -0.025], dtype=np.float32)) * ones[:, None]\n",
    "\n",
    "\n",
    "def btag_weight_variation(i_jet, jet_pt):\n",
    "    # weight variation depending on i-th jet pT (7.5% as default value, multiplied by i-th jet pT / 50 GeV)\n",
    "    return 1 + np.array([0.075, -0.075]) * (ak.singletons(jet_pt[:, i_jet]) / 50).to_numpy()\n",
    "\n",
    "\n",
    "def jet_pt_resolution(pt):\n",
    "    # normal distribution with 5% variations, shape matches jets\n",
    "    counts = ak.num(pt)\n",
    "    pt_flat = ak.flatten(pt)\n",
    "    resolution_variation = np.random.normal(np.ones_like(pt_flat), 0.05)\n",
    "    return ak.unflatten(resolution_variation, counts)\n",
    "\n",
    "\n",
    "class TtbarAnalysis(processor_base):\n",
    "    def __init__(self):\n",
    "        num_bins = 25\n",
    "        bin_low = 50\n",
    "        bin_high = 550\n",
    "        name = \"observable\"\n",
    "        label = \"observable [GeV]\"\n",
    "        self.hist = (\n",
    "            hist.Hist.new.Reg(num_bins, bin_low, bin_high, name=name, label=label)\n",
    "            .StrCat([\"4j1b\", \"4j2b\"], name=\"region\", label=\"Region\")\n",
    "            .StrCat([], name=\"process\", label=\"Process\", growth=True)\n",
    "            .StrCat([], name=\"variation\", label=\"Systematic variation\", growth=True)\n",
    "            .Weight()\n",
    "        )\n",
    "\n",
    "    def process(self, events):\n",
    "        histogram = self.hist.copy()\n",
    "\n",
    "        process = events.metadata[\"process\"]  # \"ttbar\" etc.\n",
    "        variation = events.metadata[\"variation\"]  # \"nominal\" etc.\n",
    "\n",
    "        # normalization for MC\n",
    "        x_sec = events.metadata[\"xsec\"]\n",
    "        nevts_total = events.metadata[\"nevts\"]\n",
    "        lumi = 3378 # /pb\n",
    "        if process != \"data\":\n",
    "            xsec_weight = x_sec * lumi / nevts_total\n",
    "        else:\n",
    "            xsec_weight = 1\n",
    "\n",
    "        #### systematics\n",
    "        # example of a simple flat weight variation, using the coffea nanoevents systematics feature\n",
    "        if process == \"wjets\":\n",
    "            events.add_systematic(\"scale_var\", \"UpDownSystematic\", \"weight\", flat_variation)\n",
    "\n",
    "        # jet energy scale / resolution systematics\n",
    "        # need to adjust schema to instead use coffea add_systematic feature, especially for ServiceX\n",
    "        # cannot attach pT variations to events.jet, so attach to events directly\n",
    "        # and subsequently scale pT by these scale factors\n",
    "        events[\"pt_nominal\"] = 1.0\n",
    "        events[\"pt_scale_up\"] = 1.03\n",
    "        events[\"pt_res_up\"] = jet_pt_resolution(events.jet.pt)\n",
    "\n",
    "        pt_variations = [\"pt_nominal\", \"pt_scale_up\", \"pt_res_up\"] if variation == \"nominal\" else [\"pt_nominal\"]\n",
    "        for pt_var in pt_variations:\n",
    "\n",
    "            ### event selection\n",
    "            # very very loosely based on https://arxiv.org/abs/2006.13076\n",
    "\n",
    "            # pT > 25 GeV for leptons & jets\n",
    "            selected_electrons = events.electron[events.electron.pt > 25]\n",
    "            selected_muons = events.muon[events.muon.pt > 25]\n",
    "            jet_filter = events.jet.pt * events[pt_var] > 25  # pT > 25 GeV for jets (scaled by systematic variations)\n",
    "            selected_jets = events.jet[jet_filter]\n",
    "\n",
    "            # single lepton requirement\n",
    "            event_filters = ((ak.count(selected_electrons.pt, axis=1) + ak.count(selected_muons.pt, axis=1)) == 1)\n",
    "            # at least four jets\n",
    "            pt_var_modifier = events[pt_var] if \"res\" not in pt_var else events[pt_var][jet_filter]\n",
    "            event_filters = event_filters & (ak.count(selected_jets.pt * pt_var_modifier, axis=1) >= 4)\n",
    "            # at least one b-tagged jet (\"tag\" means score above threshold)\n",
    "            B_TAG_THRESHOLD = 0.5\n",
    "            event_filters = event_filters & (ak.sum(selected_jets.btag >= B_TAG_THRESHOLD, axis=1) >= 1)\n",
    "\n",
    "            # apply event filters\n",
    "            selected_events = events[event_filters]\n",
    "            selected_electrons = selected_electrons[event_filters]\n",
    "            selected_muons = selected_muons[event_filters]\n",
    "            selected_jets = selected_jets[event_filters]\n",
    "\n",
    "            for region in [\"4j1b\", \"4j2b\"]:\n",
    "                # further filtering: 4j1b CR with single b-tag, 4j2b SR with two or more tags\n",
    "                if region == \"4j1b\":\n",
    "                    region_filter = ak.sum(selected_jets.btag >= B_TAG_THRESHOLD, axis=1) == 1\n",
    "                    selected_jets_region = selected_jets[region_filter]\n",
    "                    # use HT (scalar sum of jet pT) as observable\n",
    "                    pt_var_modifier = events[event_filters][region_filter][pt_var] if \"res\" not in pt_var else events[pt_var][jet_filter][event_filters][region_filter]\n",
    "                    observable = ak.sum(selected_jets_region.pt * pt_var_modifier, axis=-1)\n",
    "\n",
    "                elif region == \"4j2b\":\n",
    "                    region_filter = ak.sum(selected_jets.btag > B_TAG_THRESHOLD, axis=1) >= 2\n",
    "                    selected_jets_region = selected_jets[region_filter]\n",
    "\n",
    "                    if PIPELINE == \"servicex_processor\":\n",
    "                        import vector\n",
    "\n",
    "                        vector.register_awkward()\n",
    "\n",
    "                        # wrap into a four-vector object to allow addition\n",
    "                        selected_jets_region = ak.zip(\n",
    "                            {\n",
    "                                \"pt\": selected_jets_region.pt, \"eta\": selected_jets_region.eta, \"phi\": selected_jets_region.phi,\n",
    "                                \"mass\": selected_jets_region.mass, \"btag\": selected_jets_region.btag,\n",
    "                            },\n",
    "                            with_name=\"Momentum4D\",\n",
    "                        )\n",
    "\n",
    "                    # reconstruct hadronic top as bjj system with largest pT\n",
    "                    # the jet energy scale / resolution effect is not propagated to this observable at the moment\n",
    "                    trijet = ak.combinations(selected_jets_region, 3, fields=[\"j1\", \"j2\", \"j3\"])  # trijet candidates\n",
    "                    trijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3  # calculate four-momentum of tri-jet system\n",
    "                    trijet[\"max_btag\"] = np.maximum(trijet.j1.btag, np.maximum(trijet.j2.btag, trijet.j3.btag))\n",
    "                    trijet = trijet[trijet.max_btag > B_TAG_THRESHOLD]  # require at least one-btag in trijet candidates\n",
    "                    # pick trijet candidate with largest pT and calculate mass of system\n",
    "                    trijet_mass = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].mass\n",
    "                    observable = ak.flatten(trijet_mass)\n",
    "\n",
    "                ### histogram filling\n",
    "                if pt_var == \"pt_nominal\":\n",
    "                    # nominal pT, but including 2-point systematics\n",
    "                    histogram.fill(\n",
    "                            observable=observable, region=region, process=process, variation=variation, weight=xsec_weight\n",
    "                        )\n",
    "\n",
    "                    if variation == \"nominal\":\n",
    "                        # also fill weight-based variations for all nominal samples\n",
    "                        for weight_name in events.systematics.fields:\n",
    "                            for direction in [\"up\", \"down\"]:\n",
    "                                # extract the weight variations and apply all event & region filters\n",
    "                                weight_variation = events.systematics[weight_name][direction][f\"weight_{weight_name}\"][event_filters][region_filter]\n",
    "                                # fill histograms\n",
    "                                histogram.fill(\n",
    "                                    observable=observable, region=region, process=process, variation=f\"{weight_name}_{direction}\", weight=xsec_weight*weight_variation\n",
    "                                )\n",
    "\n",
    "                        # calculate additional systematics: b-tagging variations\n",
    "                        for i_var, weight_name in enumerate([f\"btag_var_{i}\" for i in range(4)]):\n",
    "                            for i_dir, direction in enumerate([\"up\", \"down\"]):\n",
    "                                # create systematic variations that depend on object properties (here: jet pT)\n",
    "                                if len(observable):\n",
    "                                    weight_variation = btag_weight_variation(i_var, selected_jets_region.pt)[:, i_dir]\n",
    "                                else:\n",
    "                                    weight_variation = 1 # no events selected\n",
    "                                histogram.fill(\n",
    "                                    observable=observable, region=region, process=process, variation=f\"{weight_name}_{direction}\", weight=xsec_weight*weight_variation\n",
    "                                )\n",
    "\n",
    "                elif variation == \"nominal\":\n",
    "                    # pT variations for nominal samples\n",
    "                    histogram.fill(\n",
    "                            observable=observable, region=region, process=process, variation=pt_var, weight=xsec_weight\n",
    "                        )\n",
    "\n",
    "        output = {\"nevents\": {events.metadata[\"dataset\"]: len(events)}, \"hist\": histogram}\n",
    "\n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b8466e-5010-4a7d-a4cb-b3960942dfbd",
   "metadata": {},
   "source": [
    "### AGC `coffea` schema\n",
    "\n",
    "When using `coffea`, we can benefit from the schema functionality to group columns into convenient objects.\n",
    "This schema is taken from [mat-adamec/agc_coffea](https://github.com/mat-adamec/agc_coffea)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e9417d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AGCSchema(BaseSchema):\n",
    "    def __init__(self, base_form):\n",
    "        super().__init__(base_form)\n",
    "        self._form[\"contents\"] = self._build_collections(self._form[\"contents\"])\n",
    "\n",
    "    def _build_collections(self, branch_forms):\n",
    "        names = set([k.split('_')[0] for k in branch_forms.keys() if not (k.startswith('number'))])\n",
    "        # Remove n(names) from consideration. It's safe to just remove names that start with n, as nothing else begins with n in our fields.\n",
    "        # Also remove GenPart, PV and MET because they deviate from the pattern of having a 'number' field.\n",
    "        names = [k for k in names if not (k.startswith('n') | k.startswith('met') | k.startswith('GenPart') | k.startswith('PV'))]\n",
    "        output = {}\n",
    "        for name in names:\n",
    "            offsets = transforms.counts2offsets_form(branch_forms['number' + name])\n",
    "            content = {k[len(name)+1:]: branch_forms[k] for k in branch_forms if (k.startswith(name + \"_\") & (k[len(name)+1:] != 'e'))}\n",
    "            # Add energy separately so its treated correctly by the p4 vector.\n",
    "            content['energy'] = branch_forms[name+'_e']\n",
    "            # Check for LorentzVector\n",
    "            output[name] = zip_forms(content, name, 'PtEtaPhiELorentzVector', offsets=offsets)\n",
    "\n",
    "        # Handle GenPart, PV, MET. Note that all the nPV_*'s should be the same. We just use one.\n",
    "        output['met'] = zip_forms({k[len('met')+1:]: branch_forms[k] for k in branch_forms if k.startswith('met_')}, 'met')\n",
    "        #output['GenPart'] = zip_forms({k[len('GenPart')+1:]: branch_forms[k] for k in branch_forms if k.startswith('GenPart_')}, 'GenPart', offsets=transforms.counts2offsets_form(branch_forms['numGenPart']))\n",
    "        output['PV'] = zip_forms({k[len('PV')+1:]: branch_forms[k] for k in branch_forms if (k.startswith('PV_') & ('npvs' not in k))}, 'PV', offsets=transforms.counts2offsets_form(branch_forms['nPV_x']))\n",
    "        return output\n",
    "\n",
    "    @property\n",
    "    def behavior(self):\n",
    "        behavior = {}\n",
    "        behavior.update(base.behavior)\n",
    "        behavior.update(vector.behavior)\n",
    "        return behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f586fab7-16cf-492b-9e00-bf8fb856bf87",
   "metadata": {
    "tags": []
   },
   "source": [
    "### \"Fileset\" construction and metadata\n",
    "\n",
    "Here, we gather all the required information about the files we want to process: paths to the files and asociated metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29341dd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processes in fileset: ['ttbar__nominal', 'ttbar__scaledown', 'ttbar__scaleup', 'ttbar__ME_var', 'ttbar__PS_var', 'single_top_s_chan__nominal', 'single_top_t_chan__nominal', 'single_top_tW__nominal', 'wjets__nominal']\n",
      "\n",
      "example of information in fileset:\n",
      "{\n",
      "  'files': [https://xrootd-local.unl.edu:1094//store/user/AGC/datasets/RunIIFall15MiniAODv2/TT_TuneCUETP8M1_13TeV-powheg-pythia8/MINIAODSIM//PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext3-v1/00000/00DF0A73-17C2-E511-B086-E41D2D08DE30.root, ...],\n",
      "  'metadata': {'process': 'ttbar', 'variation': 'nominal', 'nevts': 285620642, 'xsec': 729.84}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "fileset = utils.construct_fileset(N_FILES_MAX_PER_SAMPLE, use_xcache=False)\n",
    "\n",
    "print(f\"processes in fileset: {list(fileset.keys())}\")\n",
    "print(f\"\\nexample of information in fileset:\\n{{\\n  'files': [{fileset['ttbar__nominal']['files'][0]}, ...],\")\n",
    "print(f\"  'metadata': {fileset['ttbar__nominal']['metadata']}\\n}}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26269e8d-a0a2-45e7-a0bc-a1f618bb42f9",
   "metadata": {},
   "source": [
    "**Switching to rucio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af910ef2-fd62-4910-b086-6cdb019c21fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ttbar__nominal': {'files': ['user.ivukotic:user.ivukotic.ttbar__nominal'], 'metadata': {'process': 'ttbar', 'variation': 'nominal', 'nevts': 285620642, 'xsec': 729.84}}, 'ttbar__scaledown': {'files': ['user.ivukotic:user.ivukotic.ttbar__scaledown'], 'metadata': {'process': 'ttbar', 'variation': 'scaledown', 'nevts': 39461147, 'xsec': 729.84}}, 'ttbar__scaleup': {'files': ['user.ivukotic:user.ivukotic.ttbar__scaleup'], 'metadata': {'process': 'ttbar', 'variation': 'scaleup', 'nevts': 38472369, 'xsec': 729.84}}, 'ttbar__ME_var': {'files': ['user.ivukotic:user.ivukotic.ttbar__ME_var'], 'metadata': {'process': 'ttbar', 'variation': 'ME_var', 'nevts': 19147000, 'xsec': 729.84}}, 'ttbar__PS_var': {'files': ['user.ivukotic:user.ivukotic.ttbar__PS_var'], 'metadata': {'process': 'ttbar', 'variation': 'PS_var', 'nevts': 19383463, 'xsec': 729.84}}, 'single_top_s_chan__nominal': {'files': ['user.ivukotic:user.ivukotic.single_top_s_chan__nominal'], 'metadata': {'process': 'single_top_s_chan', 'variation': 'nominal', 'nevts': 2867199, 'xsec': 3.2944000000000004}}, 'single_top_t_chan__nominal': {'files': ['user.ivukotic:user.ivukotic.single_top_t_chan__nominal'], 'metadata': {'process': 'single_top_t_chan', 'variation': 'nominal', 'nevts': 103889916, 'xsec': 234.7936507936508}}, 'single_top_tW__nominal': {'files': ['user.ivukotic:user.ivukotic.single_top_tW__nominal'], 'metadata': {'process': 'single_top_tW', 'variation': 'nominal', 'nevts': 1999400, 'xsec': 75.842}}, 'wjets__nominal': {'files': ['user.ivukotic:user.ivukotic.wjets__nominal'], 'metadata': {'process': 'wjets', 'variation': 'nominal', 'nevts': 437222514, 'xsec': 15487.164}}}\n",
      "{'single_top_s_chan__nominal': {'files': ['user.ivukotic:user.ivukotic.single_top_s_chan__nominal'],\n",
      "                                'metadata': {'nevts': 2867199,\n",
      "                                             'process': 'single_top_s_chan',\n",
      "                                             'variation': 'nominal',\n",
      "                                             'xsec': 3.2944000000000004}},\n",
      " 'single_top_tW__nominal': {'files': ['user.ivukotic:user.ivukotic.single_top_tW__nominal'],\n",
      "                            'metadata': {'nevts': 1999400,\n",
      "                                         'process': 'single_top_tW',\n",
      "                                         'variation': 'nominal',\n",
      "                                         'xsec': 75.842}},\n",
      " 'single_top_t_chan__nominal': {'files': ['user.ivukotic:user.ivukotic.single_top_t_chan__nominal'],\n",
      "                                'metadata': {'nevts': 103889916,\n",
      "                                             'process': 'single_top_t_chan',\n",
      "                                             'variation': 'nominal',\n",
      "                                             'xsec': 234.7936507936508}},\n",
      " 'ttbar__ME_var': {'files': ['user.ivukotic:user.ivukotic.ttbar__ME_var'],\n",
      "                   'metadata': {'nevts': 19147000,\n",
      "                                'process': 'ttbar',\n",
      "                                'variation': 'ME_var',\n",
      "                                'xsec': 729.84}},\n",
      " 'ttbar__PS_var': {'files': ['user.ivukotic:user.ivukotic.ttbar__PS_var'],\n",
      "                   'metadata': {'nevts': 19383463,\n",
      "                                'process': 'ttbar',\n",
      "                                'variation': 'PS_var',\n",
      "                                'xsec': 729.84}},\n",
      " 'ttbar__nominal': {'files': ['user.ivukotic:user.ivukotic.ttbar__nominal'],\n",
      "                    'metadata': {'nevts': 285620642,\n",
      "                                 'process': 'ttbar',\n",
      "                                 'variation': 'nominal',\n",
      "                                 'xsec': 729.84}},\n",
      " 'ttbar__scaledown': {'files': ['user.ivukotic:user.ivukotic.ttbar__scaledown'],\n",
      "                      'metadata': {'nevts': 39461147,\n",
      "                                   'process': 'ttbar',\n",
      "                                   'variation': 'scaledown',\n",
      "                                   'xsec': 729.84}},\n",
      " 'ttbar__scaleup': {'files': ['user.ivukotic:user.ivukotic.ttbar__scaleup'],\n",
      "                    'metadata': {'nevts': 38472369,\n",
      "                                 'process': 'ttbar',\n",
      "                                 'variation': 'scaleup',\n",
      "                                 'xsec': 729.84}},\n",
      " 'wjets__nominal': {'files': ['user.ivukotic:user.ivukotic.wjets__nominal'],\n",
      "                    'metadata': {'nevts': 437222514,\n",
      "                                 'process': 'wjets',\n",
      "                                 'variation': 'nominal',\n",
      "                                 'xsec': 15487.164}}}\n"
     ]
    }
   ],
   "source": [
    "for sample_name in fileset.keys():\n",
    "    fileset[sample_name][\"files\"] = [f\"user.ivukotic:user.ivukotic.{sample_name}\"]\n",
    "\n",
    "# from pprint import pprint; pprint(fileset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812546fa-850e-4448-b9ee-0544be491aec",
   "metadata": {},
   "source": [
    "### ServiceX-specific functionality: query setup\n",
    "\n",
    "Define the func_adl query to be used for the purpose of extracting columns and filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ceab708d-5062-4a13-ba9a-87d762a052e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_query(source: ObjectStream) -> ObjectStream:\n",
    "    \"\"\"Query for event / column selection: no filter, select relevant lepton and jet columns\n",
    "    \"\"\"\n",
    "    return source.Where(lambda e:\\\n",
    "    # == 1 lep\n",
    "    e.electron_pt.Where(lambda pT: pT > 25).Count() + e.muon_pt.Where(lambda pT: pT > 25).Count()== 1\n",
    ")\\\n",
    ".Where(lambda e:\\\n",
    "    # >= 4 jets\n",
    "    e.jet_pt.Where(lambda pT: pT > 25).Count() >= 4\n",
    ")\\\n",
    ".Where(lambda e:\\\n",
    "    # >= 1 jet with pT > 25 GeV and b-tag >= 0.5\n",
    "    {\"pT\": e.jet_pt, \"btag\": e.jet_btag}.Zip().Where(lambda jet: jet.btag >= 0.5 and jet.pT > 25).Count() >= 1\n",
    ")\\\n",
    ".Select(lambda e:\\\n",
    "    # return columns\n",
    "    {\n",
    "        \"electron_pt\": e.electron_pt,\n",
    "        \"muon_pt\": e.muon_pt,\n",
    "        \"jet_pt\": e.jet_pt,\n",
    "        \"jet_eta\": e.jet_eta,\n",
    "        \"jet_phi\": e.jet_phi,\n",
    "        \"jet_mass\": e.jet_mass,\n",
    "        \"jet_btag\": e.jet_btag,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dbc6e8",
   "metadata": {},
   "source": [
    "### Standalone ServiceX for subsequent `coffea` processing\n",
    "\n",
    "Using `servicex-databinder`, we can execute a query and download the output.\n",
    "As the files are currently accessible through `rucio` only with ATLAS credentials, you need to use an ATLAS ServiceX instance to run this (for example via the UChicago coffea-casa analysis facility)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7004ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - retrieving data via uproot ServiceX..\n"
     ]
    },
    {
     "ename": "ServiceXException",
     "evalue": "(ServiceXException(...), 'ServiceX rejected the transformation request: (500){\"message\": \"Something went wrong\"}\\n')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mServiceXException\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m sx_db \u001b[38;5;241m=\u001b[39m DataBinder(databinder_config)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# out = sx_db.deliver(timer=True)\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m parquet_paths \u001b[38;5;241m=\u001b[39m \u001b[43msx_db\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_servicex_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexecution took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/servicex_databinder/frontend.py:72\u001b[0m, in \u001b[0;36mServiceXFrontend.get_servicex_data\u001b[0;34m(self, test_run)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n\u001b[1;32m     71\u001b[0m newloop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mget_event_loop()\n\u001b[0;32m---> 72\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mnewloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_get_my_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m log\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplete ServiceX data delivery..\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/nest_asyncio.py:89\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/asyncio/futures.py:178\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/asyncio/tasks.py:280\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/servicex_databinder/frontend.py:69\u001b[0m, in \u001b[0;36mServiceXFrontend.get_servicex_data.<locals>._get_my_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m     task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(bound_get_data(sem, sx_ds, query, sample))\n\u001b[1;32m     68\u001b[0m     tasks\u001b[38;5;241m.\u001b[39mappend(task)\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mtasks)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/asyncio/tasks.py:349\u001b[0m, in \u001b[0;36mTask.__wakeup\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 349\u001b[0m         \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    351\u001b[0m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__step(exc)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/asyncio/tasks.py:280\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/servicex_databinder/frontend.py:50\u001b[0m, in \u001b[0;36mServiceXFrontend.get_servicex_data.<locals>.bound_get_data\u001b[0;34m(sem, sx_ds, query, sample)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbound_get_data\u001b[39m(sem, sx_ds, query, sample):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m sem:\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;66;03m# return await sx_ds.get_data_parquet_async(query)\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m sx_ds\u001b[38;5;241m.\u001b[39mget_data_parquet_async(query, sample)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/servicex/servicex_utils.py:51\u001b[0m, in \u001b[0;36m_wrap_in_memory_sx_cache.<locals>.cached_version_of_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - processing request\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 51\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     52\u001b[0m     sx\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mset_inmem(h, result)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/servicex/servicex.py:363\u001b[0m, in \u001b[0;36mServiceXDataset.get_data_parquet_async\u001b[0;34m(self, selection_query, title)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(ServiceXABC\u001b[38;5;241m.\u001b[39mget_data_parquet_async, updated\u001b[38;5;241m=\u001b[39m())\n\u001b[1;32m    359\u001b[0m \u001b[38;5;129m@_wrap_in_memory_sx_cache\u001b[39m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_data_parquet_async\u001b[39m(\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28mself\u001b[39m, selection_query: \u001b[38;5;28mstr\u001b[39m, title: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    362\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Path]:\n\u001b[0;32m--> 363\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file_return(selection_query, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m, title)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/servicex/servicex.py:513\u001b[0m, in \u001b[0;36mServiceXDataset._file_return\u001b[0;34m(self, selection_query, data_format, title)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_file\u001b[39m(f: Path) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Path:\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\n\u001b[0;32m--> 513\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_return(\n\u001b[1;32m    514\u001b[0m     selection_query, convert_to_file, title, data_format\n\u001b[1;32m    515\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/backoff/_async.py:151\u001b[0m, in \u001b[0;36mretry_exception.<locals>.retry\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m details \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m: target,\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m: args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melapsed\u001b[39m\u001b[38;5;124m\"\u001b[39m: elapsed,\n\u001b[1;32m    148\u001b[0m }\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    153\u001b[0m     giveup_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m giveup(e)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/backoff/_async.py:151\u001b[0m, in \u001b[0;36mretry_exception.<locals>.retry\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m details \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m: target,\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m\"\u001b[39m: args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melapsed\u001b[39m\u001b[38;5;124m\"\u001b[39m: elapsed,\n\u001b[1;32m    148\u001b[0m }\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 151\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    153\u001b[0m     giveup_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m giveup(e)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/servicex/servicex.py:643\u001b[0m, in \u001b[0;36mServiceXDataset._data_return\u001b[0;34m(self, selection_query, converter, title, data_format)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;129m@on_exception\u001b[39m(backoff\u001b[38;5;241m.\u001b[39mconstant, ServiceXUnknownRequestID, interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, max_tries\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    610\u001b[0m \u001b[38;5;129m@on_exception\u001b[39m(\n\u001b[1;32m    611\u001b[0m     backoff\u001b[38;5;241m.\u001b[39mconstant,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    621\u001b[0m     data_format: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot-file\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    622\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;124;03m\"\"\"Given a query, return the data, in a unique order, that hold\u001b[39;00m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;124;03m    the data for the query.\u001b[39;00m\n\u001b[1;32m    625\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;124;03m                            on the converter call.\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 643\u001b[0m     all_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    644\u001b[0m         f\u001b[38;5;241m.\u001b[39mfile: f\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    645\u001b[0m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream_return(\n\u001b[1;32m    646\u001b[0m             selection_query, title, converter, data_format\n\u001b[1;32m    647\u001b[0m         )\n\u001b[1;32m    648\u001b[0m     }\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;66;03m# Finally, we need them in the proper order so we append them\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;66;03m# all together\u001b[39;00m\n\u001b[1;32m    652\u001b[0m     ordered_data \u001b[38;5;241m=\u001b[39m [all_data[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(all_data\u001b[38;5;241m.\u001b[39mkeys())]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/servicex/servicex.py:643\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;129m@on_exception\u001b[39m(backoff\u001b[38;5;241m.\u001b[39mconstant, ServiceXUnknownRequestID, interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, max_tries\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    610\u001b[0m \u001b[38;5;129m@on_exception\u001b[39m(\n\u001b[1;32m    611\u001b[0m     backoff\u001b[38;5;241m.\u001b[39mconstant,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    621\u001b[0m     data_format: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot-file\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    622\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;124;03m\"\"\"Given a query, return the data, in a unique order, that hold\u001b[39;00m\n\u001b[1;32m    624\u001b[0m \u001b[38;5;124;03m    the data for the query.\u001b[39;00m\n\u001b[1;32m    625\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;124;03m                            on the converter call.\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 643\u001b[0m     all_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    644\u001b[0m         f\u001b[38;5;241m.\u001b[39mfile: f\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m    645\u001b[0m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream_return(\n\u001b[1;32m    646\u001b[0m             selection_query, title, converter, data_format\n\u001b[1;32m    647\u001b[0m         )\n\u001b[1;32m    648\u001b[0m     }\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;66;03m# Finally, we need them in the proper order so we append them\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;66;03m# all together\u001b[39;00m\n\u001b[1;32m    652\u001b[0m     ordered_data \u001b[38;5;241m=\u001b[39m [all_data[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(all_data\u001b[38;5;241m.\u001b[39mkeys())]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/servicex/servicex.py:686\u001b[0m, in \u001b[0;36mServiceXDataset._stream_return\u001b[0;34m(self, selection_query, title, converter, data_format)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;124;03m\"\"\"Given a query, return the data, in the order it arrives back\u001b[39;00m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;124;03mconverted as appropriate.\u001b[39;00m\n\u001b[1;32m    665\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;124;03m                        on the converter call.\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    681\u001b[0m as_data \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    682\u001b[0m     StreamInfoData(f\u001b[38;5;241m.\u001b[39mfile, \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mensure_future(converter(f\u001b[38;5;241m.\u001b[39mpath)))\n\u001b[1;32m    683\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream_local_files(selection_query, title, data_format)\n\u001b[1;32m    684\u001b[0m )  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 686\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m as_data:\n\u001b[1;32m    687\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m r\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/servicex/servicex.py:681\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stream_return\u001b[39m(\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    658\u001b[0m     selection_query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    661\u001b[0m     data_format: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot-file\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    662\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncIterator[StreamInfoData]:\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;124;03m\"\"\"Given a query, return the data, in the order it arrives back\u001b[39;00m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;124;03m    converted as appropriate.\u001b[39;00m\n\u001b[1;32m    665\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;124;03m                            on the converter call.\u001b[39;00m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m     as_data \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    682\u001b[0m         StreamInfoData(f\u001b[38;5;241m.\u001b[39mfile, \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mensure_future(converter(f\u001b[38;5;241m.\u001b[39mpath)))\n\u001b[1;32m    683\u001b[0m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream_local_files(selection_query, title, data_format)\n\u001b[1;32m    684\u001b[0m     )  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m as_data:\n\u001b[1;32m    687\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m r\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/servicex/servicex.py:721\u001b[0m, in \u001b[0;36mServiceXDataset._stream_local_files\u001b[0;34m(self, selection_query, title, data_format)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# Get all the files\u001b[39;00m\n\u001b[1;32m    714\u001b[0m as_files \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    715\u001b[0m     f\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_files(\n\u001b[1;32m    717\u001b[0m         selection_query, data_format, notifier, title\n\u001b[1;32m    718\u001b[0m     )\n\u001b[1;32m    719\u001b[0m )  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m name, a_path \u001b[38;5;129;01min\u001b[39;00m as_files:\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m StreamInfoPath(name, Path(\u001b[38;5;28;01mawait\u001b[39;00m a_path))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/servicex/servicex.py:714\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    711\u001b[0m notifier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_notifier(title, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    713\u001b[0m \u001b[38;5;66;03m# Get all the files\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m as_files \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    715\u001b[0m     f\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_files(\n\u001b[1;32m    717\u001b[0m         selection_query, data_format, notifier, title\n\u001b[1;32m    718\u001b[0m     )\n\u001b[1;32m    719\u001b[0m )  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m name, a_path \u001b[38;5;129;01min\u001b[39;00m as_files:\n\u001b[1;32m    722\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m StreamInfoPath(name, Path(\u001b[38;5;28;01mawait\u001b[39;00m a_path))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/servicex/servicex.py:761\u001b[0m, in \u001b[0;36mServiceXDataset._get_files\u001b[0;34m(self, selection_query, data_type, notifier, title)\u001b[0m\n\u001b[1;32m    756\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_json_query(selection_query, data_type, title)\n\u001b[1;32m    758\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m aiohttp\u001b[38;5;241m.\u001b[39mClientSession() \u001b[38;5;28;01mas\u001b[39;00m client:\n\u001b[1;32m    759\u001b[0m \n\u001b[1;32m    760\u001b[0m     \u001b[38;5;66;03m# Get a request id - which might be cached, but if not, submit it.\u001b[39;00m\n\u001b[0;32m--> 761\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_request_id(client, query)\n\u001b[1;32m    763\u001b[0m     \u001b[38;5;66;03m# Make sure cache status exists (user could have deleted, see #176)\u001b[39;00m\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mquery_status_exists(request_id):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/servicex/servicex.py:834\u001b[0m, in \u001b[0;36mServiceXDataset._get_request_id\u001b[0;34m(self, client, query)\u001b[0m\n\u001b[1;32m    832\u001b[0m request_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mlookup_query(query)\n\u001b[1;32m    833\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 834\u001b[0m     request_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_servicex_adaptor\u001b[38;5;241m.\u001b[39msubmit_query(client, query)\n\u001b[1;32m    835\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    836\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mset_query(query, request_id)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/servicex/servicex_adaptor.py:72\u001b[0m, in \u001b[0;36mServiceXAdaptor.submit_query\u001b[0;34m(self, client, json_query)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# This was an error at ServiceX, bubble it up so code above us can\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# handle as needed.\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m response\u001b[38;5;241m.\u001b[39mtext()\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceXException(\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mServiceX rejected the transformation request: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatus\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m     )\n\u001b[1;32m     76\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[0;31mServiceXException\u001b[0m: (ServiceXException(...), 'ServiceX rejected the transformation request: (500){\"message\": \"Something went wrong\"}\\n')"
     ]
    }
   ],
   "source": [
    "if PIPELINE == \"servicex_databinder\":\n",
    "    from servicex_databinder import DataBinder\n",
    "    t0 = time.time()\n",
    "\n",
    "    # query for events with at least 4 jets with 25 GeV, at least one b-tag, and exactly one electron or muon with pT > 25 GeV\n",
    "    # returning columns required for subsequent processing\n",
    "    # query_string = \"\"\"Where(\n",
    "    #     lambda event: event.electron_pt.Where(lambda pT: pT > 25).Count() + event.muon_pt.Where(lambda pT: pT > 25).Count() == 1\n",
    "    #     ).Where(lambda event: event.jet_pt.Where(lambda pT: pT > 25).Count() >= 4\n",
    "    #     ).Where(lambda event: {\"pT\": event.jet_pt, \"btag\": event.jet_btag}.Zip().Where(lambda jet: jet.btag >= 0.5 and jet.pT > 25).Count() >= 1\n",
    "    #     ).Select(\n",
    "    #          lambda e: {\"electron_pt\": e.electron_pt, \"muon_pt\": e.muon_pt,\n",
    "    #                     \"jet_pt\": e.jet_pt, \"jet_eta\": e.jet_eta, \"jet_phi\": e.jet_phi, \"jet_mass\": e.jet_mass, \"jet_btag\": e.jet_btag}\n",
    "    # )\"\"\"\n",
    "    import inspect\n",
    "\n",
    "    query_from_source = inspect.getsource(get_query).split(\"return source.\")[-1]\n",
    "    query_string = query_from_source\n",
    "    \n",
    "    sample_names = [\"ttbar__nominal\", \"ttbar__scaledown\", \"ttbar__scaleup\", \"ttbar__ME_var\", \"ttbar__PS_var\",\n",
    "                    \"single_top_s_chan__nominal\", \"single_top_t_chan__nominal\", \"single_top_tW__nominal\", \"wjets__nominal\"]\n",
    "#    sample_names = [\"single_top_s_chan__nominal\"]  # for quick tests: small dataset with only 50 files\n",
    "    sample_list = []\n",
    "\n",
    "    for sample_name in sample_names:\n",
    "        sample_list.append({\"Name\": sample_name, \"RucioDID\": f\"user.ivukotic:user.ivukotic.{sample_name}\", \"Tree\": \"events\", \"FuncADL\": query_string})\n",
    "\n",
    "\n",
    "    databinder_config = {\n",
    "                            \"General\": {\n",
    "                                           \"ServiceXBackendName\": \"uproot\",\n",
    "                                            \"OutputDirectory\": \"outputs_databinder\",\n",
    "                                            \"OutputFormat\": \"root\",\n",
    "                                            \"IgnoreServiceXCache\": SERVICEX_IGNORE_CACHE\n",
    "                            },\n",
    "                            \"Sample\": sample_list\n",
    "                        }\n",
    "\n",
    "    sx_db = DataBinder(databinder_config)\n",
    "    # out = sx_db.deliver(timer=True)\n",
    "    parquet_paths = sx_db._sx.get_servicex_data()\n",
    "    print(f\"execution took {time.time() - t0:.2f} seconds\")\n",
    "\n",
    "    # update list of fileset files, pointing to ServiceX output for subsequent processing\n",
    "    # for process in fileset.keys():\n",
    "    #     if out.get(process):\n",
    "    #         fileset[process][\"files\"] = out[process]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f9f653-f268-4c6c-be92-abd5f772606b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update paths to point to ServiceX outputs\n",
    "for sample_name, sample_paths in zip([sample['Name'] for sample in databinder_config['Sample']], parquet_paths):\n",
    "    print(f\"updating paths for {sample_name} with {len(sample_paths)} parquet files (e.g. {sample_paths[0]}\")\n",
    "    fileset[sample_name][\"files\"] = sample_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757a0a9b-cede-4722-b1d6-98a7f4b779d7",
   "metadata": {},
   "source": [
    "### Execute the data delivery pipeline\n",
    "\n",
    "What happens here depends on the configuration setting for `PIPELINE`:\n",
    "- when set to `servicex_processor`, ServiceX will feed columns to `coffea` processors, which will asynchronously process them and accumulate the output histograms,\n",
    "- when set to `coffea`, processing will happen with pure `coffea`,\n",
    "- if `PIPELINE` was set to `servicex_databinder`, the input data has already been pre-processed and will be processed further with `coffea`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fce979",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "if PIPELINE in [\"coffea\", \"servicex_databinder\"]:\n",
    "    if USE_DASK:\n",
    "        executor = processor.DaskExecutor(client=utils.get_client(AF))\n",
    "    else:\n",
    "        executor = processor.IterativeExecutor()\n",
    "\n",
    "    from coffea.nanoevents.schemas.schema import auto_schema\n",
    "    schema = AGCSchema if PIPELINE == \"coffea\" else auto_schema\n",
    "    run = processor.Runner(executor=executor, schema=schema, savemetrics=True, metadata_cache={})\n",
    "\n",
    "    all_histograms, metrics = run(fileset, \"events\", processor_instance=TtbarAnalysis())\n",
    "    all_histograms = all_histograms[\"hist\"]\n",
    "\n",
    "elif PIPELINE == \"servicex_processor\":\n",
    "    # in a notebook:\n",
    "    all_histograms = await utils.produce_all_histograms(fileset, get_query, TtbarAnalysis, use_dask=USE_DASK, ignore_cache=SERVICEX_IGNORE_CACHE)\n",
    "\n",
    "    # as a script:\n",
    "    # async def produce_all_the_histograms():\n",
    "    #    return await utils.produce_all_histograms(fileset, get_query, TtbarAnalysis, use_dask=USE_DASK, ignore_cache=SERVICEX_IGNORE_CACHE)\n",
    "    #\n",
    "    # all_histograms = asyncio.run(produce_all_the_histograms())\n",
    " \n",
    "print(f\"\\nexecution took {time.time() - t0:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8f321a-940a-4e25-86f0-e5889331ad86",
   "metadata": {},
   "source": [
    "### Inspecting the produced histograms\n",
    "\n",
    "Let's have a look at the data we obtained.\n",
    "We built histograms in two phase space regions, for multiple physics processes and systematic variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f17abd8-74ed-40db-9ea8-73341c8bf627",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.set_style()\n",
    "\n",
    "all_histograms[120j::hist.rebin(2), \"4j1b\", :, \"nominal\"].stack(\"process\")[::-1].plot(stack=True, histtype=\"fill\", linewidth=1, edgecolor=\"grey\")\n",
    "plt.legend(frameon=False)\n",
    "plt.title(\">= 4 jets, 1 b-tag\")\n",
    "plt.xlabel(\"HT [GeV]\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318654e9-444c-401b-8957-4477d4c103f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_histograms[:, \"4j2b\", :, \"nominal\"].stack(\"process\")[::-1].plot(stack=True, histtype=\"fill\", linewidth=1,edgecolor=\"grey\")\n",
    "plt.legend(frameon=False)\n",
    "plt.title(\">= 4 jets, >= 2 b-tags\")\n",
    "plt.xlabel(\"$m_{bjj}$ [Gev]\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f90eb6-b561-43ca-a4d8-5c7f30e0cba7",
   "metadata": {},
   "source": [
    "Our top reconstruction approach ($bjj$ system with largest $p_T$) has worked!\n",
    "\n",
    "Let's also have a look at some systematic variations:\n",
    "- b-tagging, which we implemented as jet-kinematic dependent event weights,\n",
    "- jet energy variations, which vary jet kinematics, resulting in acceptance effects and observable changes.\n",
    "\n",
    "We are making of [UHI](https://uhi.readthedocs.io/) here to re-bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7843e5c0-e247-4abf-b406-263c92f797b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b-tagging variations\n",
    "all_histograms[120j::hist.rebin(2), \"4j1b\", \"ttbar\", \"nominal\"].plot(label=\"nominal\", linewidth=2)\n",
    "all_histograms[120j::hist.rebin(2), \"4j1b\", \"ttbar\", \"btag_var_0_up\"].plot(label=\"NP 1\", linewidth=2)\n",
    "all_histograms[120j::hist.rebin(2), \"4j1b\", \"ttbar\", \"btag_var_1_up\"].plot(label=\"NP 2\", linewidth=2)\n",
    "all_histograms[120j::hist.rebin(2), \"4j1b\", \"ttbar\", \"btag_var_2_up\"].plot(label=\"NP 3\", linewidth=2)\n",
    "all_histograms[120j::hist.rebin(2), \"4j1b\", \"ttbar\", \"btag_var_3_up\"].plot(label=\"NP 4\", linewidth=2)\n",
    "plt.legend(frameon=False)\n",
    "plt.xlabel(\"HT [GeV]\")\n",
    "plt.title(\"b-tagging variations\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87773be6-cabe-48a9-a9ba-7d2e33fb1b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jet energy scale variations\n",
    "all_histograms[:, \"4j2b\", \"ttbar\", \"nominal\"].plot(label=\"nominal\", linewidth=2)\n",
    "all_histograms[:, \"4j2b\", \"ttbar\", \"pt_scale_up\"].plot(label=\"scale up\", linewidth=2)\n",
    "all_histograms[:, \"4j2b\", \"ttbar\", \"pt_res_up\"].plot(label=\"resolution up\", linewidth=2)\n",
    "plt.legend(frameon=False)\n",
    "plt.xlabel(\"$m_{bjj}$ [Gev]\")\n",
    "plt.title(\"Jet energy variations\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2bb804",
   "metadata": {},
   "source": [
    "### Save histograms to disk\n",
    "\n",
    "We'll save everything to disk for subsequent usage.\n",
    "This also builds pseudo-data by combining events from the various simulation setups we have processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804ef793",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.save_histograms(all_histograms, fileset, \"histograms.root\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d731de6a-7131-4ed8-b348-4468ffa67cf0",
   "metadata": {},
   "source": [
    "### Statistical inference\n",
    "\n",
    "A statistical model has been defined in `config.yml`, ready to be used with our output.\n",
    "We will use `cabinetry` to combine all histograms into a `pyhf` workspace and fit the resulting statistical model to the pseudodata we built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486ad228",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = cabinetry.configuration.load(\"cabinetry_config.yml\")\n",
    "cabinetry.templates.collect(config)\n",
    "cabinetry.templates.postprocess(config)  # optional post-processing (e.g. smoothing)\n",
    "ws = cabinetry.workspace.build(config)\n",
    "cabinetry.workspace.save(ws, \"workspace.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c259c20-ddcc-4178-8a3e-699bd7ca2f5a",
   "metadata": {},
   "source": [
    "We can inspect the workspace with `pyhf`, or use `pyhf` to perform inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03559e2c-b8f3-4660-b23c-b7ab30545fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pyhf inspect workspace.json | head -n 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0b6564-6cb0-48d6-b57d-36b992da506b",
   "metadata": {},
   "source": [
    "Let's try out what we built: the next cell will perform a maximum likelihood fit of our statistical model to the pseudodata we built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0b4da0-79e2-47a1-8da3-3e2c3bf7947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, data = cabinetry.model_utils.model_and_data(ws)\n",
    "fit_results = cabinetry.fit.fit(model, data)\n",
    "\n",
    "cabinetry.visualize.pulls(\n",
    "    fit_results, exclude=\"ttbar_norm\", close_figure=True, save_figure=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5587b7a0-ee69-427f-b624-000ccc26ba97",
   "metadata": {},
   "source": [
    "For this pseudodata, what is the resulting ttbar cross-section divided by the Standard Model prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280d3998-b3e1-4f0d-b44c-3e7fa84558eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_index = model.config.poi_index\n",
    "print(f\"\\nfit result for ttbar_norm: {fit_results.bestfit[poi_index]:.3f} +/- {fit_results.uncertainty[poi_index]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e02bbc-f28b-4115-b8b2-c3a43cb1ccee",
   "metadata": {},
   "source": [
    "Let's also visualize the model before and after the fit, in both the regions we are using.\n",
    "The binning here corresponds to the binning used for the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf47522-414f-41f0-b3c2-2fed390469ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prediction = cabinetry.model_utils.prediction(model)\n",
    "figs = cabinetry.visualize.data_mc(model_prediction, data, close_figure=True)\n",
    "figs[0][\"figure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926c416a-e8b7-40b3-8471-9de91d217744",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs[1][\"figure\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d98fed4-5816-4318-8e2a-5a7c520ec294",
   "metadata": {},
   "source": [
    "We can see very good post-fit agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcce96d-32f3-4cb1-9dd0-81ebe63275ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prediction_postfit = cabinetry.model_utils.prediction(model, fit_results=fit_results)\n",
    "figs = cabinetry.visualize.data_mc(model_prediction_postfit, data, close_figure=True)\n",
    "figs[0][\"figure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3522f45b-d8b8-4bc8-8ee7-91e6a849b06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs[1][\"figure\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8391cff7",
   "metadata": {},
   "source": [
    "### What is next?\n",
    "\n",
    "Our next goals for this pipeline demonstration are:\n",
    "- making this analysis even **more feature-complete**,\n",
    "- **addressing performance bottlenecks** revealed by this demonstrator,\n",
    "- **collaborating** with you!\n",
    "\n",
    "Please do not hesitate to get in touch if you would like to join the effort, or are interested in re-implementing (pieces of) the pipeline with different tools!\n",
    "\n",
    "Our mailing list is analysis-grand-challenge@iris-hep.org, sign up via the [Google group](https://groups.google.com/a/iris-hep.org/g/analysis-grand-challenge)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
