{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82f3c55c-3159-4120-9705-e9ffd194b822",
   "metadata": {},
   "source": [
    "### Configuration: number of files and ServiceX usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b547996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL CONFIGURATION\n",
    "N_FILES_MAX_PER_SAMPLE = 1  # input files per process, set to -1 for no limit / 1 or 10 for quick debugging\n",
    "USE_SERVICEX = True\n",
    "USE_DASK = False\n",
    "SERVICEX_IGNORE_CACHE = False  # set to True to force re-running of transforms\n",
    "AF = \"coffea_casa\"  # set to \"coffea_casa\" for coffea-casa environments, \"EAF\" for FNAL, \"local\" for local setups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e726f38d-e04b-4b1a-8a5f-58f58e2cb329",
   "metadata": {},
   "source": [
    "### relevant imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfda82b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "\n",
    "import awkward as ak\n",
    "import vector\n",
    "from coffea import processor\n",
    "from coffea.processor import servicex\n",
    "import hist\n",
    "import json\n",
    "import numpy as np\n",
    "import uproot\n",
    "import cabinetry\n",
    "from func_adl_servicex import ServiceXSourceUpROOT\n",
    "from func_adl import ObjectStream\n",
    "from servicex import ServiceXDataset\n",
    "from coffea.nanoevents.schemas.base import BaseSchema\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utils\n",
    "\n",
    "vector.register_awkward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8674317-8ecd-402c-a1fe-d5de1074705e",
   "metadata": {},
   "source": [
    "### coffea Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "790dc3c2-7311-4ad4-a177-4fd4a1553559",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "processor_base = processor.ProcessorABC if not USE_SERVICEX else servicex.Analysis\n",
    "\n",
    "class TtbarAnalysis(processor_base):\n",
    "    def __init__(self):\n",
    "        num_bins = 25\n",
    "        bin_low = 50\n",
    "        bin_high = 550\n",
    "        name = \"observable\"\n",
    "        label = \"observable [GeV]\"\n",
    "        self.hist = (\n",
    "            hist.Hist.new.Reg(num_bins, bin_low, bin_high, name=name, label=label)\n",
    "            .StrCat([\"4j1b\", \"4j2b\"], name=\"region\", label=\"Region\")\n",
    "            .StrCat([], name=\"process\", label=\"Process\", growth=True)\n",
    "            .StrCat([], name=\"variation\", label=\"Systematic variation\", growth=True)\n",
    "            .Weight()\n",
    "        )\n",
    "\n",
    "    def process(self, events):\n",
    "        histogram = self.hist.copy()\n",
    "\n",
    "        process = events.metadata[\"process\"]  # \"ttbar\" etc.\n",
    "        variation = events.metadata[\"variation\"]  # \"nominal\" etc.\n",
    "\n",
    "        # normalization for MC\n",
    "        x_sec = events.metadata[\"xsec\"]\n",
    "        nevts_total = events.metadata[\"nevts\"]\n",
    "        lumi = 3378 # /pb\n",
    "        if process != \"data\":\n",
    "            xsec_weight = x_sec * lumi / nevts_total\n",
    "        else:\n",
    "            xsec_weight = 1\n",
    "            \n",
    "        def some_event_weight(ones):\n",
    "            return (1.0 + np.array([0.05, -0.05], dtype=np.float32)) * ones[:, None]\n",
    "\n",
    "        events.add_systematic(\"ScaleVar_1\", \"UpDownSystematic\", \"weight\", some_event_weight)\n",
    "        events.add_systematic(\"ScaleVar_2\", \"UpDownSystematic\", \"weight\", some_event_weight)\n",
    "\n",
    "        # very very loosely based on https://arxiv.org/abs/2006.13076\n",
    "\n",
    "        # pT > 25 GeV for leptons & jets\n",
    "        selected_electrons = events.electron[events.electron.pt > 25]\n",
    "        selected_muons = events.muon[events.muon.pt > 25]\n",
    "        selected_jets = events.jet[events.jet.pt > 25]  # pT > 25 GeV for jets\n",
    "\n",
    "        # single lepton requirement\n",
    "        event_filters = (ak.count(selected_electrons.pt, axis=1) & ak.count(selected_muons.pt, axis=1) == 1)\n",
    "        # at least four jets\n",
    "        event_filters = event_filters & (ak.count(selected_jets.pt, axis=1) >= 4)\n",
    "        # at least one b-tagged jet (\"tag\" means score above threshold)\n",
    "        B_TAG_THRESHOLD = 0.5\n",
    "        event_filters = event_filters & (ak.sum(selected_jets.btag >= B_TAG_THRESHOLD, axis=1) >= 1)\n",
    "\n",
    "        # apply event filters\n",
    "        selected_events = events[event_filters]\n",
    "        selected_electrons = selected_electrons[event_filters]\n",
    "        selected_muons = selected_muons[event_filters]\n",
    "        selected_jets = selected_jets[event_filters]\n",
    "\n",
    "        for region in [\"4j1b\", \"4j2b\"]:\n",
    "            # further filtering: 4j1b CR with single b-tag, 4j2b SR with two or more tags\n",
    "            if region == \"4j1b\":\n",
    "                region_filter = ak.sum(selected_jets.btag >= B_TAG_THRESHOLD, axis=1) == 1\n",
    "                selected_jets_region = selected_jets[region_filter]\n",
    "                # use HT (scalar sum of jet pT) as observable\n",
    "                observable = ak.sum(selected_jets_region.pt, axis=-1)\n",
    "        \n",
    "            elif region == \"4j2b\":\n",
    "                region_filter = ak.sum(selected_jets.btag > B_TAG_THRESHOLD, axis=1) >= 2\n",
    "                selected_jets_region = selected_jets[region_filter]\n",
    "                \n",
    "                if USE_SERVICEX:\n",
    "                    # wrap into a four-vector object to allow addition\n",
    "                    selected_jets_region = ak.zip(\n",
    "                        {\n",
    "                            \"px\": selected_jets_region.px, \"py\": selected_jets_region.py, \"pz\": selected_jets_region.pz,\n",
    "                            \"E\": selected_jets_region.e, \"btag\": selected_jets_region.btag,\n",
    "                        },\n",
    "                        with_name=\"Momentum4D\",\n",
    "                    )\n",
    "                \n",
    "                # reconstruct hadronic top as bjj system with largest pT\n",
    "                trijet = ak.combinations(selected_jets_region, 3, fields=[\"j1\", \"j2\", \"j3\"])  # trijet candidates\n",
    "                trijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3  # calculate four-momentum of tri-jet system\n",
    "                trijet[\"max_btag\"] = np.maximum(trijet.j1.btag, np.maximum(trijet.j2.btag, trijet.j3.btag))\n",
    "                trijet = trijet[trijet.max_btag > B_TAG_THRESHOLD]  # require at least one-btag in trijet candidates\n",
    "                # pick trijet candidate with largest pT and calculate mass of system\n",
    "                trijet_mass = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].mass\n",
    "                observable = ak.flatten(trijet_mass)\n",
    "\n",
    "            if variation == \"nominal\":\n",
    "                histogram.fill(\n",
    "                    observable=observable, region=region, process=process, variation=variation, weight=xsec_weight\n",
    "                )\n",
    "                \n",
    "                # also fill weight-based variations\n",
    "                for weight_name in events.systematics.fields:\n",
    "                    for direction in [\"up\", \"down\"]:\n",
    "                        # extract the weight variations and apply all event & region filters\n",
    "                        weight_variation = events.systematics[weight_name][direction][f\"weight_{weight_name}\"][event_filters][region_filter]\n",
    "                        # fill histograms\n",
    "                        histogram.fill(\n",
    "                            observable=observable, region=region, process=process, variation=f\"{weight_name}_{direction}\", weight=xsec_weight*weight_variation\n",
    "                        )\n",
    "\n",
    "        output = {\"nevents\": {events.metadata[\"dataset\"]: len(events)}, \"hist\": histogram}\n",
    "\n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b8466e-5010-4a7d-a4cb-b3960942dfbd",
   "metadata": {},
   "source": [
    "### AGC schema (for pure coffea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e9417d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from coffea.nanoevents.schemas.base import BaseSchema, zip_forms\n",
    "from coffea.nanoevents.methods import base, vector\n",
    "from coffea.nanoevents import transforms\n",
    "\n",
    "\n",
    "# https://github.com/mat-adamec/agc_coffea/blob/main/agc_schema.py\n",
    "class AGCSchema(BaseSchema):\n",
    "    def __init__(self, base_form):\n",
    "        super().__init__(base_form)\n",
    "        self._form[\"contents\"] = self._build_collections(self._form[\"contents\"])\n",
    "\n",
    "    def _build_collections(self, branch_forms):\n",
    "        names = set([k.split('_')[0] for k in branch_forms.keys() if not (k.startswith('number'))])\n",
    "        # Remove n(names) from consideration. It's safe to just remove names that start with n, as nothing else begins with n in our fields.\n",
    "        # Also remove GenPart, PV and MET because they deviate from the pattern of having a 'number' field.\n",
    "        names = [k for k in names if not (k.startswith('n') | k.startswith('met') | k.startswith('GenPart') | k.startswith('PV'))]\n",
    "        output = {}\n",
    "        for name in names:\n",
    "            offsets = transforms.counts2offsets_form(branch_forms['number' + name])\n",
    "            content = {k[len(name)+1:]: branch_forms[k] for k in branch_forms if (k.startswith(name + \"_\") & (k[len(name)+1:] != 'e'))}\n",
    "            # Add energy separately so its treated correctly by the p4 vector.\n",
    "            content['energy'] = branch_forms[name+'_e']\n",
    "            # Check for LorentzVector\n",
    "            output[name] = zip_forms(content, name, 'PtEtaPhiELorentzVector', offsets=offsets)\n",
    "\n",
    "        # Handle GenPart, PV, MET. Note that all the nPV_*'s should be the same. We just use one.\n",
    "        output['met'] = zip_forms({k[len('met')+1:]: branch_forms[k] for k in branch_forms if k.startswith('met_')}, 'met')\n",
    "        #output['GenPart'] = zip_forms({k[len('GenPart')+1:]: branch_forms[k] for k in branch_forms if k.startswith('GenPart_')}, 'GenPart', offsets=transforms.counts2offsets_form(branch_forms['numGenPart']))\n",
    "        output['PV'] = zip_forms({k[len('PV')+1:]: branch_forms[k] for k in branch_forms if (k.startswith('PV_') & ('npvs' not in k))}, 'PV', offsets=transforms.counts2offsets_form(branch_forms['nPV_x']))\n",
    "        return output\n",
    "\n",
    "    @property\n",
    "    def behavior(self):\n",
    "        behavior = {}\n",
    "        behavior.update(base.behavior)\n",
    "        behavior.update(vector.behavior)\n",
    "        return behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f586fab7-16cf-492b-9e00-bf8fb856bf87",
   "metadata": {},
   "source": [
    "### metadata & fileset construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29341dd9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processes in fileset: ['ttbar__nominal', 'ttbar__scaledown', 'ttbar__scaleup', 'ttbar__ME_var', 'ttbar__PS_var', 'single_top_s_chan__nominal', 'single_top_t_chan__nominal', 'single_top_tW__nominal', 'wjets__nominal']\n",
      "\n",
      "example of information in fileset:\n",
      "{\n",
      "  'files': [https://xrootd-local.unl.edu:1094//store/user/AGC/datasets/RunIIFall15MiniAODv2/TT_TuneCUETP8M1_13TeV-powheg-pythia8/MINIAODSIM//PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext3-v1/00000/00DF0A73-17C2-E511-B086-E41D2D08DE30.root, ...],\n",
      "  'metadata': {'process': 'ttbar', 'variation': 'nominal', 'nevts': 41812, 'xsec': 729.84}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "fileset = utils.construct_fileset(N_FILES_MAX_PER_SAMPLE, use_xcache=False)\n",
    "\n",
    "print(f\"processes in fileset: {list(fileset.keys())}\")\n",
    "print(f\"\\nexample of information in fileset:\\n{{\\n  'files': [{fileset['ttbar__nominal']['files'][0]}, ...],\")\n",
    "print(f\"  'metadata': {fileset['ttbar__nominal']['metadata']}\\n}}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812546fa-850e-4448-b9ee-0544be491aec",
   "metadata": {},
   "source": [
    "### ServiceX-specific functionality: datasource construction, query, asyncio\n",
    "(move to utils file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceab708d-5062-4a13-ba9a-87d762a052e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_datasource(fileset:dict, name: str, query: ObjectStream, ignore_cache: bool):\n",
    "    \"\"\"Creates a ServiceX datasource for a particular ATLAS Open data file.\"\"\"\n",
    "    datasets = [ServiceXDataset(fileset[name][\"files\"], backend_name=\"uproot\", ignore_cache=ignore_cache)]\n",
    "    return servicex.DataSource(\n",
    "        query=query, metadata=fileset[name][\"metadata\"], datasets=datasets\n",
    "    )\n",
    "\n",
    "def get_query(source: ObjectStream) -> ObjectStream:\n",
    "    \"\"\"Query for event / column selection: no filter, select all columns\n",
    "    \"\"\"\n",
    "    return source.Select(lambda e: {\n",
    "                                    \"electron_pt\": e.electron_pt,\n",
    "                                    \"muon_pt\": e.muon_pt,\n",
    "                                    \"jet_e\": e.jet_e,\n",
    "                                    \"jet_pt\": e.jet_pt,\n",
    "                                    \"jet_px\": e.jet_px,\n",
    "                                    \"jet_py\": e.jet_py,\n",
    "                                    \"jet_pz\": e.jet_pz,\n",
    "                                    \"jet_eta\": e.jet_eta,\n",
    "                                    \"jet_phi\": e.jet_phi,\n",
    "                                    \"jet_mass\": e.jet_mass,\n",
    "                                    \"jet_btag\": e.jet_btag,\n",
    "                                   }\n",
    "                        )\n",
    "\n",
    "\n",
    "async def produce_all_histograms(fileset, use_dask=False):\n",
    "    \"\"\"Runs the histogram production, processing input files with ServiceX and\n",
    "    producing histograms with coffea.\n",
    "    \"\"\"\n",
    "    # create the query\n",
    "    ds = ServiceXSourceUpROOT(\"cernopendata://dummy\", \"events\", backend_name=\"uproot\")\n",
    "    ds.return_qastle = True\n",
    "    data_query = get_query(ds)\n",
    "\n",
    "    # EXECUTOR: local vs Dask\n",
    "    if not use_dask:\n",
    "        executor = servicex.LocalExecutor()\n",
    "    else:\n",
    "        executor = servicex.DaskExecutor(client_addr=\"tls://localhost:8786\")\n",
    "\n",
    "    datasources = [\n",
    "        make_datasource(fileset, ds_name, data_query, ignore_cache=SERVICEX_IGNORE_CACHE)\n",
    "        for ds_name in fileset.keys()\n",
    "    ]\n",
    "\n",
    "    # create the analysis processor\n",
    "    analysis_processor = TtbarAnalysis()\n",
    "\n",
    "    async def run_updates_stream(accumulator_stream, name):\n",
    "        \"\"\"Run to get the last item in the stream\"\"\"\n",
    "        coffea_info = None\n",
    "        try:\n",
    "            async for coffea_info in accumulator_stream:\n",
    "                pass\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Failure while processing {name}\") from e\n",
    "        return coffea_info\n",
    "\n",
    "    all_histogram_dicts = await asyncio.gather(\n",
    "        *[\n",
    "            run_updates_stream(\n",
    "                executor.execute(analysis_processor, source),\n",
    "                f\"{source.metadata['process']}__{source.metadata['variation']}\",\n",
    "            )\n",
    "            for source in datasources\n",
    "        ]\n",
    "    )\n",
    "    all_histograms = sum([h[\"hist\"] for h in all_histogram_dicts])\n",
    "    \n",
    "    return all_histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dbc6e8",
   "metadata": {},
   "source": [
    "### via servicex-databinder (separate ServiceX + coffea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d7004ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from servicex_databinder import DataBinder\n",
    "query_string = \"\"\"Select(lambda e: {\n",
    "                    'electron_pt': e.electron_pt,\n",
    "                    'jet_e': e.jet_e,\n",
    "                    'jet_pt': e.jet_pt,\n",
    "                    'jet_px': e.jet_px,\n",
    "                    'jet_py': e.jet_py,\n",
    "                    'jet_pz': e.jet_pz,\n",
    "                    'jet_eta': e.jet_eta,\n",
    "                    'jet_phi': e.jet_phi,\n",
    "                    'jet_mass': e.jet_mass,\n",
    "                    'jet_btag': e.jet_btag})\"\"\"\n",
    "\n",
    "# from servicex_databinder import DataBinder\n",
    "\n",
    "databinder_config = {\"General\": {\"ServiceXBackendName\": \"uproot\",\n",
    "                                 \"OutputDirectory\": \"outputs\",\n",
    "                                 \"OutputFormat\": \"parquet\",\n",
    "                                 \"IgnoreServiceXCache\": True},\n",
    "                     \"Sample\": [\n",
    "                         {\n",
    "                             \"Name\": \"single_top_s_chan__nominal\",\n",
    "                             \"RucioDID\": \"user.ivukotic:user.ivukotic.single_top_s_chan__nominal\",\n",
    "                             \"Tree\": \"events\",\n",
    "                             \"FuncADL\": query_string\n",
    "                         }\n",
    "                     ]\n",
    "                    }\n",
    "\n",
    "# sx_db = DataBinder(databinder_config)\n",
    "# out = sx_db.deliver()\n",
    "\n",
    "# update list of fileset files, pointing to ServiceX output\n",
    "# for process in fileset.keys():\n",
    "#     fileset[process][\"files\"] = out[process][\"events\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757a0a9b-cede-4722-b1d6-98a7f4b779d7",
   "metadata": {},
   "source": [
    "### execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78fce979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "execution took 1.73 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "if USE_SERVICEX:\n",
    "    # in a notebook:\n",
    "    all_histograms = await produce_all_histograms(fileset, use_dask=USE_DASK)\n",
    "\n",
    "    # as a script:\n",
    "    # async def produce_all_the_histograms(fileset, use_dask=use_dask):\n",
    "    #    return await produce_all_histograms(fileset, use_dask=use_dask)\n",
    "    #\n",
    "    # all_histograms = asyncio.run(produce_all_the_histograms(fileset, use_dask=USE_DASK))\n",
    "\n",
    "else:\n",
    "    t0 = time.time()\n",
    "    if USE_DASK:\n",
    "        executor = processor.DaskExecutor(client=utils.get_client(AF))\n",
    "    else:\n",
    "        executor = processor.IterativeExecutor()\n",
    "\n",
    "    run = processor.Runner(executor=executor, schema=AGCSchema, savemetrics=True, metadata_cache={})\n",
    "\n",
    "    all_histograms, metrics = run(fileset, \"events\", processor_instance=TtbarAnalysis())\n",
    "    all_histograms = all_histograms[\"hist\"]\n",
    "    \n",
    "print(f\"\\nexecution took {time.time() - t0:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "414a2d3a-3578-4a00-a91a-66fbb18225c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_SERVICEX:\n",
    "    print(f\"number of files processed: {metrics['chunks']}\")\n",
    "    print(f\"data read: {metrics['bytesread']/1024**2:.1f} MB\")\n",
    "\n",
    "    metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67f5e846-ca12-413b-bc4f-0925ad09f99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  58.96392232 1179.27844638 3832.65495073 3301.97964986 3773.69102841\n",
      " 4658.1498632  4776.07770784 2004.77335884 2240.62904812 2004.77335884\n",
      " 1709.95374725]\n",
      "[  61.91211319 1238.24226379 4024.28735733 3467.07833862 3962.37524414\n",
      " 4891.05694199 5014.88116837 2105.01184845 2352.66030121 2105.01184845\n",
      " 1795.4512825 ]\n",
      "[  56.01572418 1120.31448364 3641.02207184 3136.8805542  3585.00634766\n",
      " 4425.24221039 4537.27365875 1904.53462219 2128.59751892 1904.53462219\n",
      " 1624.45600128]\n"
     ]
    }
   ],
   "source": [
    "utils.set_style()\n",
    "\n",
    "print(all_histograms[120j::hist.rebin(2), \"4j1b\", \"ttbar\", \"nominal\"].values())\n",
    "print(all_histograms[120j::hist.rebin(2), \"4j1b\", \"ttbar\", \"ScaleVar_1_up\"].values())\n",
    "print(all_histograms[120j::hist.rebin(2), \"4j1b\", \"ttbar\", \"ScaleVar_1_down\"].values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8f321a-940a-4e25-86f0-e5889331ad86",
   "metadata": {},
   "source": [
    "### plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f17abd8-74ed-40db-9ea8-73341c8bf627",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.set_style()\n",
    "\n",
    "all_histograms[120j::hist.rebin(2), \"4j1b\", :, \"nominal\"].stack(\"process\")[::-1].plot(stack=True, histtype=\"fill\", linewidth=1, edgecolor=\"grey\")\n",
    "plt.legend(frameon=False)\n",
    "plt.title(\">= 4 jets, 1 b-tag\")\n",
    "plt.xlabel(\"HT [GeV]\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318654e9-444c-401b-8957-4477d4c103f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_histograms[:, \"4j2b\", :, \"nominal\"].stack(\"process\")[::-1].plot(stack=True, histtype=\"fill\", linewidth=1,edgecolor=\"grey\")\n",
    "plt.legend(frameon=False)\n",
    "plt.title(\">= 4 jets, >= 2 b-tags\");\n",
    "plt.xlabel(\"$m_{bjj}$ [Gev]\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a157582-a4c4-4a48-af34-03b27826e626",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"4j, >=2b\", sum(all_histograms[:, \"4j2b\", \"ttbar\", \"nominal\"].values())) # total before splitting: 101790.4947274355\n",
    "print(\"4j, 1b\", sum(all_histograms[:, \"4j1b\", \"ttbar\", \"nominal\"].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad19aaae-2ffa-4e28-8efb-4f94bf1fdb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_histograms[:, \"4j1b\", \"ttbar\", :].stack(\"variation\").plot(linewidth=2)\n",
    "plt.legend()\n",
    "plt.title(\"ttbar systematic variations in 4j1b\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e796239",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_histograms[:, \"4j2b\", \"ttbar\", :].stack(\"variation\").plot(linewidth=2)\n",
    "plt.legend()\n",
    "plt.title(\"ttbar systematic variations in 4j2b\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2bb804",
   "metadata": {},
   "source": [
    "### saving histograms & cabinetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804ef793",
   "metadata": {},
   "outputs": [],
   "source": [
    "with uproot.recreate(\"histograms.root\") as f:\n",
    "    for region in [\"4j1b\", \"4j2b\"]:\n",
    "        f[f\"{region}_ttbar\"] = all_histograms[120j::hist.rebin(2), region, \"ttbar\", \"nominal\"]\n",
    "        f[f\"{region}_ttbar_ME_var\"] = all_histograms[120j::hist.rebin(2), region, \"ttbar\", \"ME_var\"]\n",
    "        f[f\"{region}_ttbar_PS_var\"] = all_histograms[120j::hist.rebin(2), region, \"ttbar\", \"PS_var\"]\n",
    "        f[f\"{region}_ttbar_scaledown\"] = all_histograms[120j::hist.rebin(2), region, \"ttbar\", \"scaledown\"]\n",
    "        f[f\"{region}_ttbar_scaleup\"] = all_histograms[120j::hist.rebin(2), region, \"ttbar\", \"scaleup\"]\n",
    "\n",
    "        f[f\"{region}_wjets\"] = all_histograms[120j::hist.rebin(2), region, \"wjets\", \"nominal\"]\n",
    "        f[f\"{region}_single_top_s_chan\"] = all_histograms[120j::hist.rebin(2), region, \"single_top_s_chan\", \"nominal\"]\n",
    "        f[f\"{region}_single_top_t_chan\"] = all_histograms[120j::hist.rebin(2), region, \"single_top_t_chan\", \"nominal\"]\n",
    "        f[f\"{region}_single_top_tW\"] = all_histograms[120j::hist.rebin(2), region, \"single_top_tW\", \"nominal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486ad228",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = cabinetry.configuration.load(\"config.yml\")\n",
    "cabinetry.templates.collect(config)\n",
    "cabinetry.templates.postprocess(config)  # optional post-processing (e.g. smoothing)\n",
    "ws = cabinetry.workspace.build(config)\n",
    "\n",
    "model, data = cabinetry.model_utils.model_and_data(ws)\n",
    "fit_results = cabinetry.fit.fit(model, data)\n",
    "\n",
    "cabinetry.visualize.pulls(\n",
    "    fit_results, exclude=\"ttbar_norm\", close_figure=True, save_figure=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf47522-414f-41f0-b3c2-2fed390469ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prediction = cabinetry.model_utils.prediction(model)\n",
    "figs = cabinetry.visualize.data_mc(model_prediction, data, close_figure=True, log_scale=False)\n",
    "figs[0][\"figure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926c416a-e8b7-40b3-8471-9de91d217744",
   "metadata": {},
   "outputs": [],
   "source": [
    "figs[1][\"figure\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcce96d-32f3-4cb1-9dd0-81ebe63275ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
