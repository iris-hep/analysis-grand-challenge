{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f643dd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"error\")\n",
    "\n",
    "# ---\n",
    "# jupyter:\n",
    "#   jupytext:\n",
    "#     formats: ipynb,py:percent\n",
    "#     text_representation:\n",
    "#       extension: .py\n",
    "#       format_name: percent\n",
    "#       format_version: '1.3'\n",
    "#       jupytext_version: 1.14.5\n",
    "#   kernelspec:\n",
    "#     display_name: Python 3 (ipykernel)\n",
    "#     language: python\n",
    "#     name: python3\n",
    "# ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40fe12f",
   "metadata": {},
   "source": [
    "# CMS Open Data $t\\bar{t}$: from data delivery to statistical inference\n",
    "\n",
    "We are using [2015 CMS Open Data](https://cms.cern/news/first-cms-open-data-lhc-run-2-released) in this demonstration to showcase an analysis pipeline.\n",
    "It features data delivery and processing, histogram construction and visualization, as well as statistical inference.\n",
    "\n",
    "This notebook was developed in the context of the [IRIS-HEP AGC tools 2022 workshop](https://indico.cern.ch/e/agc-tools-2).\n",
    "This work was supported by the U.S. National Science Foundation (NSF) Cooperative Agreement OAC-1836650 (IRIS-HEP).\n",
    "\n",
    "This is a **technical demonstration**.\n",
    "We are including the relevant workflow aspects that physicists need in their work, but we are not focusing on making every piece of the demonstration physically meaningful.\n",
    "This concerns in particular systematic uncertainties: we capture the workflow, but the actual implementations are more complex in practice.\n",
    "If you are interested in the physics side of analyzing top pair production, check out the latest results from [ATLAS](https://twiki.cern.ch/twiki/bin/view/AtlasPublic/TopPublicResults) and [CMS](https://cms-results.web.cern.ch/cms-results/public-results/preliminary-results/)!\n",
    "If you would like to see more technical demonstrations, also check out an [ATLAS Open Data example](https://indico.cern.ch/event/1076231/contributions/4560405/) demonstrated previously.\n",
    "\n",
    "This notebook implements most of the analysis pipeline shown in the following picture, using the tools also mentioned there:\n",
    "![ecosystem visualization](utils/ecosystem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f59a138",
   "metadata": {},
   "source": [
    "### Data pipelines\n",
    "\n",
    "There are two possible pipelines: one with `ServiceX` enabled, and one using only `coffea` for processing.\n",
    "![processing pipelines](utils/processing_pipelines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b92c4c",
   "metadata": {},
   "source": [
    "### Imports: setting up our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2130f252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "\n",
    "import awkward as ak\n",
    "# import cabinetry\n",
    "from coffea import processor\n",
    "from coffea.nanoevents import NanoAODSchema\n",
    "# from servicex import ServiceXDataset\n",
    "# from func_adl import ObjectStream\n",
    "# from func_adl_servicex import ServiceXSourceUpROOT\n",
    "\n",
    "import hist\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import uproot\n",
    "\n",
    "import utils  # contains code for bookkeeping and cosmetics, as well as some boilerplate\n",
    "\n",
    "# logging.getLogger(\"cabinetry\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f3c55c-3159-4120-9705-e9ffd194b822",
   "metadata": {},
   "source": [
    "### Configuration: number of files and data delivery path\n",
    "\n",
    "The number of files per sample set here determines the size of the dataset we are processing.\n",
    "There are 9 samples being used here, all part of the 2015 CMS Open Data release.\n",
    "They are pre-converted from miniAOD files into ntuple format, similar to nanoAODs.\n",
    "More details about the inputs can be found [here](https://github.com/iris-hep/analysis-grand-challenge/tree/main/datasets/cms-open-data-2015).\n",
    "\n",
    "The table below summarizes the amount of data processed depending on the `N_FILES_MAX_PER_SAMPLE` setting.\n",
    "\n",
    "| setting | number of files | total size |\n",
    "| --- | --- | --- |\n",
    "| `1` | 12 | 25.1 GB |\n",
    "| `2` | 24 | 46.5 GB |\n",
    "| `5` | 52 | 110 GB |\n",
    "| `10` | 88 | 205 GB |\n",
    "| `20` | 149 | 364 GB |\n",
    "| `50` | 264 | 636 GB |\n",
    "| `100` | 404 | 965 GB |\n",
    "| `200` | 604 | 1.40 TB |\n",
    "| `-1` | 796 | 1.78 TB |\n",
    "\n",
    "The input files are all in the 1â€“3 GB range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b547996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GLOBAL CONFIGURATION\n",
    "\n",
    "# input files per process, set to e.g. 10 (smaller number = faster)\n",
    "N_FILES_MAX_PER_SAMPLE = 1\n",
    "\n",
    "# enable Dask\n",
    "USE_DASK = False\n",
    "\n",
    "# enable ServiceX\n",
    "USE_SERVICEX = False\n",
    "\n",
    "# ServiceX: ignore cache with repeated queries\n",
    "SERVICEX_IGNORE_CACHE = False\n",
    "\n",
    "# analysis facility: set to \"coffea_casa\" for coffea-casa environments, \"EAF\" for FNAL, \"local\" for local setups\n",
    "AF = \"coffea_casa\"\n",
    "\n",
    "\n",
    "### BENCHMARKING-SPECIFIC SETTINGS\n",
    "\n",
    "# chunk size to use\n",
    "CHUNKSIZE = 500_000\n",
    "\n",
    "# metadata to propagate through to metrics\n",
    "AF_NAME = \"coffea_casa\"  # \"ssl-dev\" allows for the switch to local data on /data\n",
    "SYSTEMATICS = \"all\"  # currently has no effect\n",
    "CORES_PER_WORKER = 2  # does not do anything, only used for metric gathering (set to 2 for distributed coffea-casa)\n",
    "\n",
    "# scaling for local setups with FuturesExecutor\n",
    "NUM_CORES = 4\n",
    "\n",
    "# only I/O, all other processing disabled\n",
    "DISABLE_PROCESSING = False\n",
    "\n",
    "# read additional branches (only with DISABLE_PROCESSING = True)\n",
    "# acceptable values are 2.7, 4, 15, 25, 50 (corresponding to % of file read), 2.7% corresponds to the standard branches used in the notebook\n",
    "IO_FILE_PERCENT = 2.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8674317-8ecd-402c-a1fe-d5de1074705e",
   "metadata": {},
   "source": [
    "### Defining our `coffea` Processor\n",
    "\n",
    "The processor includes a lot of the physics analysis details:\n",
    "- event filtering and the calculation of observables,\n",
    "- event weighting,\n",
    "- calculating systematic uncertainties at the event and object level,\n",
    "- filling all the information into histograms that get aggregated and ultimately returned to us by `coffea`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce9f9967",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import hist.dask\n",
    "import dask_awkward as dak\n",
    "\n",
    "\n",
    "# functions creating systematic variations\n",
    "def flat_variation(ones):\n",
    "    # 2.5% weight variations\n",
    "    return (1.0 + np.array([0.025, -0.025], dtype=np.float32)) * ones[:, None]\n",
    "\n",
    "\n",
    "def btag_weight_variation(i_jet, jet_pt):\n",
    "    # weight variation depending on i-th jet pT (7.5% as default value, multiplied by i-th jet pT / 50 GeV)\n",
    "    return 1 + np.array([0.075, -0.075]) * (dak.singletons(jet_pt[:, i_jet]) / 50).to_numpy()\n",
    "\n",
    "\n",
    "def jet_pt_resolution(pt):\n",
    "    # normal distribution with 5% variations, shape matches jets\n",
    "    counts = dak.num(pt)\n",
    "    pt_flat = dak.flatten(pt)\n",
    "    resolution_variation = np.random.normal(dak.ones_like(pt_flat), 0.05)\n",
    "    return dak.unflatten(resolution_variation, counts)\n",
    "\n",
    "\n",
    "class TtbarAnalysis(processor.ProcessorABC):\n",
    "    def __init__(self, disable_processing, io_file_percent):\n",
    "        num_bins = 25\n",
    "        bin_low = 50\n",
    "        bin_high = 550\n",
    "        name = \"observable\"\n",
    "        label = \"observable [GeV]\"\n",
    "        self.hist = (\n",
    "            hist.dask.Hist.new.Reg(num_bins, bin_low, bin_high, name=name, label=label)\n",
    "            .StrCat([\"4j1b\", \"4j2b\"], name=\"region\", label=\"Region\")\n",
    "            .StrCat([], name=\"process\", label=\"Process\", growth=True)\n",
    "            .StrCat([], name=\"variation\", label=\"Systematic variation\", growth=True)\n",
    "            .Weight()\n",
    "        )\n",
    "        self.disable_processing = disable_processing\n",
    "        self.io_file_percent = io_file_percent\n",
    "\n",
    "    def only_do_IO(self, events):\n",
    "        # standard AGC branches cover 2.7% of the data\n",
    "            branches_to_read = []\n",
    "            if self.io_file_percent >= 2.7:\n",
    "                branches_to_read.extend([\"Jet_pt\", \"Jet_eta\", \"Jet_phi\", \"Jet_btagCSVV2\", \"Jet_mass\", \"Muon_pt\", \"Electron_pt\"])\n",
    "\n",
    "            if self.io_file_percent >= 4:\n",
    "                branches_to_read.extend([\"Electron_phi\", \"Electron_eta\",\"Electron_mass\",\"Muon_phi\",\"Muon_eta\",\"Muon_mass\",\n",
    "                                         \"Photon_pt\",\"Photon_eta\",\"Photon_mass\",\"Jet_jetId\"])\n",
    "\n",
    "            if self.io_file_percent>=15:\n",
    "                branches_to_read.extend([\"Jet_nConstituents\",\"Jet_electronIdx1\",\"Jet_electronIdx2\",\"Jet_muonIdx1\",\"Jet_muonIdx2\",\n",
    "                                         \"Jet_chHEF\",\"Jet_area\",\"Jet_puId\",\"Jet_qgl\",\"Jet_btagDeepB\",\"Jet_btagDeepCvB\",\n",
    "                                         \"Jet_btagDeepCvL\",\"Jet_btagDeepFlavB\",\"Jet_btagDeepFlavCvB\",\"Jet_btagDeepFlavCvL\",\n",
    "                                         \"Jet_btagDeepFlavQG\",\"Jet_chEmEF\",\"Jet_chFPV0EF\",\"Jet_muEF\",\"Jet_muonSubtrFactor\",\n",
    "                                         \"Jet_neEmEF\",\"Jet_neHEF\",\"Jet_puIdDisc\"])\n",
    "\n",
    "            if self.io_file_percent>=25:\n",
    "                branches_to_read.extend([\"GenPart_pt\",\"GenPart_eta\",\"GenPart_phi\",\"GenPart_mass\",\"GenPart_genPartIdxMother\",\n",
    "                                         \"GenPart_pdgId\",\"GenPart_status\",\"GenPart_statusFlags\"])\n",
    "\n",
    "            if self.io_file_percent==50:\n",
    "                branches_to_read.extend([\"Jet_rawFactor\",\"Jet_bRegCorr\",\"Jet_bRegRes\",\"Jet_cRegCorr\",\"Jet_cRegRes\",\"Jet_nElectrons\",\n",
    "                                         \"Jet_nMuons\",\"GenJet_pt\",\"GenJet_eta\",\"GenJet_phi\",\"GenJet_mass\",\"Tau_pt\",\"Tau_eta\",\"Tau_mass\",\n",
    "                                         \"Tau_phi\",\"Muon_dxy\",\"Muon_dxyErr\",\"Muon_dxybs\",\"Muon_dz\",\"Muon_dzErr\",\"Electron_dxy\",\n",
    "                                         \"Electron_dxyErr\",\"Electron_dz\",\"Electron_dzErr\",\"Electron_eInvMinusPInv\",\"Electron_energyErr\",\n",
    "                                         \"Electron_hoe\",\"Electron_ip3d\",\"Electron_jetPtRelv2\",\"Electron_jetRelIso\",\n",
    "                                         \"Electron_miniPFRelIso_all\",\"Electron_miniPFRelIso_chg\",\"Electron_mvaFall17V2Iso\",\n",
    "                                         \"Electron_mvaFall17V2noIso\",\"Electron_pfRelIso03_all\",\"Electron_pfRelIso03_chg\",\"Electron_r9\",\n",
    "                                         \"Electron_scEtOverPt\",\"Electron_sieie\",\"Electron_sip3d\",\"Electron_mvaTTH\",\"Electron_charge\",\n",
    "                                         \"Electron_cutBased\",\"Electron_jetIdx\",\"Electron_pdgId\",\"Electron_photonIdx\",\"Electron_tightCharge\"])\n",
    "\n",
    "            if self.io_file_percent not in [2.7, 4, 15, 25, 50]:\n",
    "                raise NotImplementedError(\"supported values for I/O percentage are 2.7, 4, 15, 25, 50\")\n",
    "\n",
    "            for branch in branches_to_read:\n",
    "                if \"_\" in branch:\n",
    "                    split = branch.split(\"_\")\n",
    "                    object_type = split[0]\n",
    "                    property_name = '_'.join(split[1:])\n",
    "                    dak.materialized(events[object_type][property_name])\n",
    "                else:\n",
    "                    dak.materialized(events[branch])\n",
    "            return {\"hist\": {}}\n",
    "\n",
    "    def process(self, events):\n",
    "        if self.disable_processing:\n",
    "            # IO testing with no subsequent processing\n",
    "            return self.only_do_IO(events)\n",
    "\n",
    "        histogram = self.hist.copy()\n",
    "\n",
    "        process = events.metadata[\"process\"]  # \"ttbar\" etc.\n",
    "        variation = events.metadata[\"variation\"]  # \"nominal\" etc.\n",
    "\n",
    "        # normalization for MC\n",
    "        x_sec = events.metadata[\"xsec\"]\n",
    "        nevts_total = events.metadata[\"nevts\"]\n",
    "        lumi = 3378 # /pb\n",
    "        if process != \"data\":\n",
    "            xsec_weight = x_sec * lumi / nevts_total\n",
    "        else:\n",
    "            xsec_weight = 1\n",
    "\n",
    "        #### systematics\n",
    "        # example of a simple flat weight variation, using the coffea nanoevents systematics feature\n",
    "        # if process == \"wjets\":\n",
    "        #     events.add_systematic(\"scale_var\", \"UpDownSystematic\", \"weight\", flat_variation)\n",
    "\n",
    "        # jet energy scale / resolution systematics\n",
    "        # need to adjust schema to instead use coffea add_systematic feature, especially for ServiceX\n",
    "        # cannot attach pT variations to events.jet, so attach to events directly\n",
    "        # and subsequently scale pT by these scale factors\n",
    "        events[\"pt_nominal\"] = dak.ones_like(events.MET.pt)\n",
    "        events[\"pt_scale_up\"] = dak.ones_like(events.MET.pt)*1.03\n",
    "        events[\"pt_res_up\"] = dak.ones_like(events.MET.pt) #jet_pt_resolution(events.Jet.pt)\n",
    "\n",
    "\n",
    "        pt_variations = [\"pt_nominal\", \"pt_scale_up\", \"pt_res_up\"] if variation == \"nominal\" else [\"pt_nominal\"]\n",
    "        for pt_var in pt_variations:\n",
    "\n",
    "            ### event selection\n",
    "            # very very loosely based on https://arxiv.org/abs/2006.13076\n",
    "\n",
    "            # pT > 25 GeV for leptons & jets\n",
    "            selected_electrons = events.Electron[(events.Electron.pt>25)]\n",
    "            selected_muons = events.Muon[(events.Muon.pt >25)]\n",
    "            jet_filter = (events.Jet.pt * events[pt_var]) > 25\n",
    "            selected_jets = events.Jet[jet_filter]\n",
    "\n",
    "            # single lepton requirement\n",
    "            event_filters = ((dak.count(selected_electrons.pt, axis=1) + dak.count(selected_muons.pt, axis=1)) == 1)\n",
    "            # at least four jets\n",
    "            pt_var_modifier = dak.ones_like(selected_jets.pt)  # events[pt_var] if \"res\" not in pt_var else events[pt_var][jet_filter]\n",
    "            event_filters = event_filters & (dak.count(selected_jets.pt * pt_var_modifier, axis=1) >= 4)\n",
    "            # at least one b-tagged jet (\"tag\" means score above threshold)\n",
    "            B_TAG_THRESHOLD = 0.5\n",
    "            event_filters = event_filters & (dak.sum(selected_jets.btagCSVV2 >= B_TAG_THRESHOLD, axis=1) >= 1)\n",
    "\n",
    "            # apply event filters\n",
    "            selected_events = events[event_filters]\n",
    "            selected_electrons = selected_electrons[event_filters]\n",
    "            selected_muons = selected_muons[event_filters]\n",
    "            selected_jets = selected_jets[event_filters]\n",
    "\n",
    "            for region in [\"4j1b\", \"4j2b\"]:\n",
    "                # further filtering: 4j1b CR with single b-tag, 4j2b SR with two or more tags\n",
    "                if region == \"4j1b\":\n",
    "                    region_filter = dak.sum(selected_jets.btagCSVV2 >= B_TAG_THRESHOLD, axis=1) == 1\n",
    "                    selected_jets_region = selected_jets[region_filter]\n",
    "                    # use HT (scalar sum of jet pT) as observable\n",
    "                    pt_var_modifier = (\n",
    "                        events[event_filters][region_filter][pt_var]\n",
    "                        # if \"res\" not in pt_var\n",
    "                        # else events[pt_var][jet_filter][event_filters][region_filter]\n",
    "                    )\n",
    "                    observable = dak.sum(selected_jets_region.pt * pt_var_modifier, axis=-1)\n",
    "\n",
    "                elif region == \"4j2b\":\n",
    "                    region_filter = dak.sum(selected_jets.btagCSVV2 > B_TAG_THRESHOLD, axis=1) >= 2\n",
    "                    selected_jets_region = selected_jets[region_filter]\n",
    "\n",
    "                    # reconstruct hadronic top as bjj system with largest pT\n",
    "                    # the jet energy scale / resolution effect is not propagated to this observable at the moment\n",
    "                    trijet = dak.combinations(selected_jets_region, 3, fields=[\"j1\", \"j2\", \"j3\"])  # trijet candidates\n",
    "                    trijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3  # calculate four-momentum of tri-jet system\n",
    "                    trijet[\"max_btag\"] = np.maximum(trijet.j1.btagCSVV2, np.maximum(trijet.j2.btagCSVV2, trijet.j3.btagCSVV2))\n",
    "                    trijet = trijet[trijet.max_btag > B_TAG_THRESHOLD]  # at least one-btag in trijet candidates\n",
    "                    # pick trijet candidate with largest pT and calculate mass of system\n",
    "                    trijet_mass = trijet[\"p4\"][dak.argmax(trijet.p4.pt, axis=1, keepdims=True)].mass\n",
    "                    observable = dak.flatten(trijet_mass)\n",
    "\n",
    "                ### histogram filling\n",
    "                if pt_var == \"pt_nominal\":\n",
    "                    # nominal pT, but including 2-point systematics\n",
    "                    histogram.fill(\n",
    "                            observable=observable, region=region, process=process,\n",
    "                            variation=variation, weight=dak.ones_like(observable)*xsec_weight\n",
    "                        )\n",
    "\n",
    "                    if variation == \"nominal\":\n",
    "                        # also fill weight-based variations for all nominal samples\n",
    "                        for weight_name in []: # events.systematics.fields:\n",
    "                            for direction in [\"up\", \"down\"]:\n",
    "                                # extract the weight variations and apply all event & region filters\n",
    "                                weight_variation = events.systematics[weight_name][direction][\n",
    "                                    f\"weight_{weight_name}\"][event_filters][region_filter]\n",
    "                                # fill histograms\n",
    "                                histogram.fill(\n",
    "                                    observable=observable, region=region, process=process,\n",
    "                                    variation=f\"{weight_name}_{direction}\", weight=dak.ones_like(observable)*xsec_weight*weight_variation\n",
    "                                )\n",
    "\n",
    "                        # calculate additional systematics: b-tagging variations\n",
    "                        for i_var, weight_name in enumerate([f\"btag_var_{i}\" for i in range(4)]):\n",
    "                            for i_dir, direction in enumerate([\"up\", \"down\"]):\n",
    "                                # create systematic variations that depend on object properties (here: jet pT)\n",
    "                                if len(observable):\n",
    "                                    weight_variation = dak.ones_like(observable) #btag_weight_variation(i_var, selected_jets_region.pt)[:, i_dir]\n",
    "                                else:\n",
    "                                    weight_variation = 1 # no events selected\n",
    "                                histogram.fill(\n",
    "                                    observable=observable, region=region, process=process,\n",
    "                                    variation=f\"{weight_name}_{direction}\", weight=dak.ones_like(observable)*xsec_weight*weight_variation\n",
    "                                )\n",
    "\n",
    "                elif variation == \"nominal\":\n",
    "                    # pT variations for nominal samples\n",
    "                    histogram.fill(\n",
    "                            observable=observable, region=region, process=process,\n",
    "                            variation=pt_var, weight=dak.ones_like(observable)*xsec_weight\n",
    "                        )\n",
    "\n",
    "        output = {\"nevents\": {events.metadata[\"dataset\"]: len(events)}, \"hist\": histogram}\n",
    "\n",
    "        return output\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f586fab7-16cf-492b-9e00-bf8fb856bf87",
   "metadata": {},
   "source": [
    "### \"Fileset\" construction and metadata\n",
    "\n",
    "Here, we gather all the required information about the files we want to process: paths to the files and asociated metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29341dd9",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processes in fileset: ['ttbar__nominal', 'ttbar__scaledown', 'ttbar__scaleup', 'ttbar__ME_var', 'ttbar__PS_var', 'single_top_s_chan__nominal', 'single_top_t_chan__nominal', 'single_top_tW__nominal', 'wjets__nominal']\n",
      "\n",
      "example of information in fileset:\n",
      "{\n",
      "  'files': [https://xrootd-local.unl.edu:1094//store/user/AGC/datasets/merged/TT_TuneCUETP8M1_13TeV-powheg-pythia8/1.root, ...],\n",
      "  'metadata': {'process': 'ttbar', 'variation': 'nominal', 'nevts': 442122, 'xsec': 729.84}\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "fileset = utils.construct_fileset(N_FILES_MAX_PER_SAMPLE, use_xcache=False, af_name=AF_NAME)  # local files on /data for ssl-dev\n",
    "\n",
    "print(f\"processes in fileset: {list(fileset.keys())}\")\n",
    "print(f\"\\nexample of information in fileset:\\n{{\\n  'files': [{fileset['ttbar__nominal']['files'][0]}, ...],\")\n",
    "print(f\"  'metadata': {fileset['ttbar__nominal']['metadata']}\\n}}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "821ea6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileset = {\"ttbar__nominal\": fileset[\"ttbar__nominal\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6164a27b-0bca-451c-a066-913df8db823e",
   "metadata": {},
   "source": [
    "### ServiceX-specific functionality: query setup\n",
    "\n",
    "Define the func_adl query to be used for the purpose of extracting columns and filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b7583ea-a2eb-48b4-a511-26c1cdf65a32",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# def get_query(source: ObjectStream) -> ObjectStream:\n",
    "#     \"\"\"Query for event / column selection: >=4j >=1b, ==1 lep with pT>25 GeV, return relevant columns\n",
    "#     \"\"\"\n",
    "#     return source.Where(lambda e: e.Electron_pt.Where(lambda pt: pt > 25).Count()\n",
    "#                         + e.Muon_pt.Where(lambda pt: pt > 25).Count() == 1)\\\n",
    "#                  .Where(lambda e: e.Jet_pt.Where(lambda pt: pt > 25).Count() >= 4)\\\n",
    "#                  .Where(lambda g: {\"pt\": g.Jet_pt,\n",
    "#                                    \"btagCSVV2\": g.Jet_btagCSVV2}.Zip().Where(lambda jet:\n",
    "#                                                                              jet.btagCSVV2 >= 0.5 \n",
    "#                                                                              and jet.pt > 25).Count() >= 1)\\\n",
    "#                  .Select(lambda f: {\"Electron_pt\": f.Electron_pt,\n",
    "#                                     \"Muon_pt\": f.Muon_pt,\n",
    "#                                     \"Jet_mass\": f.Jet_mass,\n",
    "#                                     \"Jet_pt\": f.Jet_pt,\n",
    "#                                     \"Jet_eta\": f.Jet_eta,\n",
    "#                                     \"Jet_phi\": f.Jet_phi,\n",
    "#                                     \"Jet_btagCSVV2\": f.Jet_btagCSVV2,\n",
    "#                                    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909bb129-3ab7-47cd-80ea-64c42410ed51",
   "metadata": {},
   "source": [
    "### Caching the queried datasets with `ServiceX`\n",
    "\n",
    "Using the queries created with `func_adl`, we are using `ServiceX` to read the CMS Open Data files to build cached files with only the specific event information as dictated by the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cc3c8bf-7b6b-4d67-8326-804271549c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_SERVICEX:\n",
    "\n",
    "    # dummy dataset on which to generate the query\n",
    "    dummy_ds = ServiceXSourceUpROOT(\"cernopendata://dummy\", \"Events\", backend_name=\"uproot\")\n",
    "\n",
    "    # tell low-level infrastructure not to contact ServiceX yet, only to\n",
    "    # return the qastle string it would have sent\n",
    "    dummy_ds.return_qastle = True\n",
    "\n",
    "    # create the query\n",
    "    query = get_query(dummy_ds).value()\n",
    "\n",
    "    # now we query the files and create a fileset dictionary containing the\n",
    "    # URLs pointing to the queried files\n",
    "\n",
    "    t0 = time.time()\n",
    "    for process in fileset.keys():\n",
    "        ds = ServiceXDataset(fileset[process]['files'],\n",
    "                             backend_name=\"uproot\",\n",
    "                             ignore_cache=SERVICEX_IGNORE_CACHE)\n",
    "        files = ds.get_data_rootfiles_uri(query,\n",
    "                                          as_signed_url=True,\n",
    "                                          title=process)\n",
    "\n",
    "\n",
    "        fileset[process][\"files\"] = [f.url for f in files]\n",
    "\n",
    "    print(f\"ServiceX data delivery took {time.time() - t0:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757a0a9b-cede-4722-b1d6-98a7f4b779d7",
   "metadata": {},
   "source": [
    "### Execute the data delivery pipeline\n",
    "\n",
    "What happens here depends on the flag `USE_SERVICEX`. If set to true, the processor is run on the data previously gathered by ServiceX, then will gather output histograms.\n",
    "\n",
    "When `USE_SERVICEX` is false, the input files need to be processed during this step as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78fce979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "260293cadcec473ea03ff17a02ea7dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3184be8042949ee83adaeb1b7ba3417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "AGCSchema.__init__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m filemeta \u001b[38;5;241m=\u001b[39m run\u001b[38;5;241m.\u001b[39mpreprocess(fileset, treename\u001b[38;5;241m=\u001b[39mtreename)  \u001b[38;5;66;03m# pre-processing\u001b[39;00m\n\u001b[1;32m     17\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m---> 18\u001b[0m all_histograms, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtreename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTtbarAnalysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDISABLE_PROCESSING\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIO_FILE_PERCENT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# processing\u001b[39;00m\n\u001b[1;32m     19\u001b[0m exec_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m     21\u001b[0m all_histograms \u001b[38;5;241m=\u001b[39m all_histograms[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhist\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/mambaforge/envs/coffea_2023/lib/python3.10/site-packages/coffea/processor/executor.py:1768\u001b[0m, in \u001b[0;36mRunner.__call__\u001b[0;34m(self, fileset, treename, processor_instance)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1748\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1749\u001b[0m     fileset: Dict,\n\u001b[1;32m   1750\u001b[0m     treename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   1751\u001b[0m     processor_instance: ProcessorABC,\n\u001b[1;32m   1752\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Accumulatable:\n\u001b[1;32m   1753\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run the processor_instance on a given fileset\u001b[39;00m\n\u001b[1;32m   1754\u001b[0m \n\u001b[1;32m   1755\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;124;03m            An instance of a class deriving from ProcessorABC\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1768\u001b[0m     wrapped_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtreename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_dataframes:\n\u001b[1;32m   1770\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_out  \u001b[38;5;66;03m# not wrapped anymore\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/coffea_2023/lib/python3.10/site-packages/coffea/processor/executor.py:1927\u001b[0m, in \u001b[0;36mRunner.run\u001b[0;34m(self, fileset, processor_instance, treename)\u001b[0m\n\u001b[1;32m   1922\u001b[0m closure \u001b[38;5;241m=\u001b[39m partial(\n\u001b[1;32m   1923\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautomatic_retries, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretries, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskipbadfiles, closure\n\u001b[1;32m   1924\u001b[0m )\n\u001b[1;32m   1926\u001b[0m executor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecutor\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mexe_args)\n\u001b[0;32m-> 1927\u001b[0m wrapped_out, e \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrapped_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo chunks returned results, verify ``processor`` instance structure.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;124m        if you used skipbadfiles=True, it is possible all your files are bad.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1932\u001b[0m     )\n",
      "File \u001b[0;32m~/mambaforge/envs/coffea_2023/lib/python3.10/site-packages/coffea/processor/executor.py:671\u001b[0m, in \u001b[0;36mIterativeExecutor.__call__\u001b[0;34m(self, items, function, accumulator)\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m rich_bar() \u001b[38;5;28;01mas\u001b[39;00m progress:\n\u001b[1;32m    667\u001b[0m     p_id \u001b[38;5;241m=\u001b[39m progress\u001b[38;5;241m.\u001b[39madd_task(\n\u001b[1;32m    668\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdesc, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(items), unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus\n\u001b[1;32m    669\u001b[0m     )\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 671\u001b[0m         \u001b[43maccumulate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitems\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtask_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mp_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m            \u001b[49m\u001b[43maccumulator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    679\u001b[0m         \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    680\u001b[0m     )\n",
      "File \u001b[0;32m~/mambaforge/envs/coffea_2023/lib/python3.10/site-packages/coffea/processor/accumulator.py:109\u001b[0m, in \u001b[0;36maccumulate\u001b[0;34m(items, accum)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accum \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 109\u001b[0m         accum \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;66;03m# we want to produce a new object so that the input is not mutated\u001b[39;00m\n\u001b[1;32m    111\u001b[0m         accum \u001b[38;5;241m=\u001b[39m add(accum, \u001b[38;5;28mnext\u001b[39m(gen))\n",
      "File \u001b[0;32m~/mambaforge/envs/coffea_2023/lib/python3.10/site-packages/coffea/processor/accumulator.py:106\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21maccumulate\u001b[39m(\n\u001b[1;32m    104\u001b[0m     items: Iterable[Optional[Accumulatable]], accum: Optional[Accumulatable] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    105\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Accumulatable]:\n\u001b[0;32m--> 106\u001b[0m     gen \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m items \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m accum \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/coffea_2023/lib/python3.10/site-packages/rich/progress.py:1216\u001b[0m, in \u001b[0;36mProgress.track\u001b[0;34m(self, sequence, total, task_id, description, update_period)\u001b[0m\n\u001b[1;32m   1214\u001b[0m advance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance\n\u001b[1;32m   1215\u001b[0m refresh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefresh\n\u001b[0;32m-> 1216\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m sequence:\n\u001b[1;32m   1217\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m value\n\u001b[1;32m   1218\u001b[0m     advance(task_id, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/mambaforge/envs/coffea_2023/lib/python3.10/site-packages/coffea/processor/executor.py:1368\u001b[0m, in \u001b[0;36mRunner.automatic_retries\u001b[0;34m(retries, skipbadfiles, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1362\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1363\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1364\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m skipbadfiles\n\u001b[1;32m   1365\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAuth failed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m chain)\n\u001b[1;32m   1366\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m retries \u001b[38;5;241m==\u001b[39m retry_count\n\u001b[1;32m   1367\u001b[0m     ):\n\u001b[0;32m-> 1368\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1369\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempt \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (retry_count \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, retries \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   1370\u001b[0m retry_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/mambaforge/envs/coffea_2023/lib/python3.10/site-packages/coffea/processor/executor.py:1337\u001b[0m, in \u001b[0;36mRunner.automatic_retries\u001b[0;34m(retries, skipbadfiles, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m retry_count \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m retries:\n\u001b[1;32m   1336\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1337\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1338\u001b[0m     \u001b[38;5;66;03m# catch xrootd errors and optionally skip\u001b[39;00m\n\u001b[1;32m   1339\u001b[0m     \u001b[38;5;66;03m# or retry to read the file\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/mambaforge/envs/coffea_2023/lib/python3.10/site-packages/coffea/processor/executor.py:1683\u001b[0m, in \u001b[0;36mRunner._work_function\u001b[0;34m(format, xrootdtimeout, mmap, schema, cache_function, use_dataframes, savemetrics, item, processor_instance)\u001b[0m\n\u001b[1;32m   1673\u001b[0m     materialized \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1674\u001b[0m     factory \u001b[38;5;241m=\u001b[39m NanoEventsFactory\u001b[38;5;241m.\u001b[39mfrom_root(\n\u001b[1;32m   1675\u001b[0m         file\u001b[38;5;241m=\u001b[39mfile,\n\u001b[1;32m   1676\u001b[0m         treepath\u001b[38;5;241m=\u001b[39mitem\u001b[38;5;241m.\u001b[39mtreename,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1681\u001b[0m         permit_dask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1682\u001b[0m     )\n\u001b[0;32m-> 1683\u001b[0m     events \u001b[38;5;241m=\u001b[39m \u001b[43mfactory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[item\u001b[38;5;241m.\u001b[39mentrystart : item\u001b[38;5;241m.\u001b[39mentrystop]\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparquet\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1685\u001b[0m     skyhook_options \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/mambaforge/envs/coffea_2023/lib/python3.10/site-packages/coffea/nanoevents/factory.py:682\u001b[0m, in \u001b[0;36mNanoEventsFactory.events\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Build events\"\"\"\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_dask:\n\u001b[0;32m--> 682\u001b[0m     events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mapping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mform_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    683\u001b[0m     events\u001b[38;5;241m.\u001b[39mbehavior[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__original_array__\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m weakref\u001b[38;5;241m.\u001b[39mref(events)\n\u001b[1;32m    684\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m events\n",
      "File \u001b[0;32m~/mambaforge/envs/coffea_2023/lib/python3.10/site-packages/uproot/_dask.py:206\u001b[0m, in \u001b[0;36mdask\u001b[0;34m(files, filter_name, filter_typename, filter_branch, recursive, full_paths, step_size, library, ak_add_doc, custom_classes, allow_missing, open_files, form_mapping, **options)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _get_dak_array(\n\u001b[1;32m    192\u001b[0m             files,\n\u001b[1;32m    193\u001b[0m             filter_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    203\u001b[0m             form_mapping,\n\u001b[1;32m    204\u001b[0m         )\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 206\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_get_dak_array_delay_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilter_typename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfilter_branch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfull_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcustom_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_missing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreal_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m            \u001b[49m\u001b[43minterp_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m            \u001b[49m\u001b[43mform_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n",
      "File \u001b[0;32m~/mambaforge/envs/coffea_2023/lib/python3.10/site-packages/uproot/_dask.py:962\u001b[0m, in \u001b[0;36m_get_dak_array_delay_open\u001b[0;34m(files, filter_name, filter_typename, filter_branch, recursive, full_paths, custom_classes, allow_missing, real_options, interp_options, form_mapping)\u001b[0m\n\u001b[1;32m    951\u001b[0m obj \u001b[38;5;241m=\u001b[39m uproot\u001b[38;5;241m.\u001b[39m_util\u001b[38;5;241m.\u001b[39mregularize_object_path(\n\u001b[1;32m    952\u001b[0m     ffile_path, fobject_path, custom_classes, allow_missing, real_options\n\u001b[1;32m    953\u001b[0m )\n\u001b[1;32m    954\u001b[0m common_keys \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mkeys(\n\u001b[1;32m    955\u001b[0m     recursive\u001b[38;5;241m=\u001b[39mrecursive,\n\u001b[1;32m    956\u001b[0m     filter_name\u001b[38;5;241m=\u001b[39mfilter_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    959\u001b[0m     full_paths\u001b[38;5;241m=\u001b[39mfull_paths,\n\u001b[1;32m    960\u001b[0m )\n\u001b[0;32m--> 962\u001b[0m meta, form \u001b[38;5;241m=\u001b[39m \u001b[43m_get_meta_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mawkward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdask_awkward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommon_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43mform_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterp_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mak_add_doc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dask_awkward\u001b[38;5;241m.\u001b[39mfrom_map(\n\u001b[1;32m    972\u001b[0m     _UprootOpenAndRead(\n\u001b[1;32m    973\u001b[0m         custom_classes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    985\u001b[0m     meta\u001b[38;5;241m=\u001b[39mmeta,\n\u001b[1;32m    986\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/envs/coffea_2023/lib/python3.10/site-packages/uproot/_dask.py:782\u001b[0m, in \u001b[0;36m_get_meta_array\u001b[0;34m(awkward, dask_awkward, ttree, common_keys, form_mapping, ak_add_doc)\u001b[0m\n\u001b[1;32m    779\u001b[0m form \u001b[38;5;241m=\u001b[39m awkward\u001b[38;5;241m.\u001b[39mforms\u001b[38;5;241m.\u001b[39mRecordForm(contents, common_keys, parameters\u001b[38;5;241m=\u001b[39mparameters)\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m form_mapping \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 782\u001b[0m     form \u001b[38;5;241m=\u001b[39m \u001b[43mform_mapping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    784\u001b[0m empty_arr \u001b[38;5;241m=\u001b[39m form\u001b[38;5;241m.\u001b[39mlength_zero_array(\n\u001b[1;32m    785\u001b[0m     behavior\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m form_mapping \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m form_mapping\u001b[38;5;241m.\u001b[39mbehavior\n\u001b[1;32m    786\u001b[0m )\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dask_awkward\u001b[38;5;241m.\u001b[39mcore\u001b[38;5;241m.\u001b[39mtypetracer_array(empty_arr), form\n",
      "File \u001b[0;32m~/mambaforge/envs/coffea_2023/lib/python3.10/site-packages/coffea/nanoevents/factory.py:127\u001b[0m, in \u001b[0;36m_map_schema_uproot.__call__\u001b[0;34m(self, form)\u001b[0m\n\u001b[1;32m    114\u001b[0m     branch_forms[field] \u001b[38;5;241m=\u001b[39m _lazify_form(\n\u001b[1;32m    115\u001b[0m         iform, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfield\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,!load\u001b[39m\u001b[38;5;124m\"\u001b[39m, docstr\u001b[38;5;241m=\u001b[39miform[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__doc__\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    116\u001b[0m     )\n\u001b[1;32m    117\u001b[0m lform \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecordArray\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontents\u001b[39m\u001b[38;5;124m\"\u001b[39m: [item \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m branch_forms\u001b[38;5;241m.\u001b[39mvalues()],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mform_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    126\u001b[0m }\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m awkward\u001b[38;5;241m.\u001b[39mforms\u001b[38;5;241m.\u001b[39mform\u001b[38;5;241m.\u001b[39mfrom_dict(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschemaclass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mform)\n",
      "\u001b[0;31mTypeError\u001b[0m: AGCSchema.__init__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    NanoAODSchema.warn_missing_crossrefs = False # silences warnings about branches we will not use here\n",
    "    if USE_DASK:\n",
    "        executor = processor.DaskExecutor(client=utils.get_client(AF))\n",
    "    else:\n",
    "        # executor = processor.FuturesExecutor(workers=NUM_CORES)\n",
    "        executor = processor.IterativeExecutor()\n",
    "\n",
    "    run = processor.Runner(executor=executor, schema=NanoAODSchema, savemetrics=True, metadata_cache={}, chunksize=CHUNKSIZE)\n",
    "\n",
    "    if USE_SERVICEX:\n",
    "        treename = \"servicex\"\n",
    "\n",
    "    else:\n",
    "        treename = \"Events\"\n",
    "\n",
    "    filemeta = run.preprocess(fileset, treename=treename)  # pre-processing\n",
    "\n",
    "    t0 = time.monotonic()\n",
    "    all_histograms, metrics = run(fileset, treename, processor_instance=TtbarAnalysis(DISABLE_PROCESSING, IO_FILE_PERCENT))  # processing\n",
    "    breakpoint()\n",
    "    exec_time = time.monotonic() - t0\n",
    "\n",
    "    all_histograms = all_histograms[\"hist\"]\n",
    "\n",
    "    print(f\"\\nexecution took {exec_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
