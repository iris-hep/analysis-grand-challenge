{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71b3fd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this notebook to work, you need to set up your environment as specified in the readme of https://github.com/gordonwatts/pyhep-2021-SX-OpenDataDemo\n",
    "\n",
    "import asyncio, os\n",
    "import numpy as np\n",
    "from coffea.processor.servicex import Analysis, DataSource, LocalExecutor, DaskExecutor\n",
    "from func_adl import ObjectStream\n",
    "from func_adl_servicex import ServiceXSourceUpROOT\n",
    "from servicex.servicex import ServiceXDataset\n",
    "import hist\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import infofile # Same infofile used in https://github.com/atlas-outreach-data-tools/notebooks-collection-opendata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24197a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'root://eospublic.cern.ch//eos/opendata/atlas/OutreachDatasets/2020-01-22/4lep/'\n",
    "\n",
    "fileset = {'Data': [\n",
    "                    prefix + 'Data/data_A.4lep.root',\n",
    "                    prefix + 'Data/data_B.4lep.root',\n",
    "                    prefix + 'Data/data_C.4lep.root',\n",
    "                    prefix + 'Data/data_D.4lep.root'\n",
    "                    ],\n",
    "           'Background $Z,tt^{bar}$': [\n",
    "                                       prefix + 'MC/mc_361106.Zee.4lep.root',\n",
    "                                       prefix + 'MC/mc_361107.Zmumu.4lep.root',\n",
    "                                       prefix + 'MC/mc_410000.ttbar_lep.4lep.root'\n",
    "                                       ],\n",
    "           'Background $ZZ^{star}$': [prefix + 'MC/mc_363490.llll.4lep.root'],\n",
    "           'Signal ($m_H$ = 125 GeV)': [\n",
    "                                        prefix + 'MC/mc_345060.ggH125_ZZ4lep.4lep.root',\n",
    "                                        prefix + 'MC/mc_344235.VBFH125_ZZ4lep.4lep.root',\n",
    "                                        prefix + 'MC/mc_341964.WH125_ZZ4lep.4lep.root',\n",
    "                                        prefix + 'MC/mc_341947.ZH125_ZZ4lep.4lep.root'\n",
    "                                       ]}\n",
    "\n",
    "xsec_map = infofile.infos  # dictionary with event weighting information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be2a6ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method from https://github.com/atlas-outreach-data-tools/notebooks-collection-opendata\n",
    "\n",
    "lumi = 10 # fb-1 # data_A,data_B,data_C,data_D\n",
    "\n",
    "def get_xsec_weight(sample):\n",
    "    info = xsec_map[sample]\n",
    "    xsec_weight = (lumi*1000*info[\"xsec\"])/(info[\"sumw\"]*info[\"red_eff\"]) #*1000 to go from fb-1 to pb-1\n",
    "    return xsec_weight # return cross-section weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3cd3ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from method from https://github.com/atlas-outreach-data-tools/notebooks-collection-opendata\n",
    "\n",
    "def calc_mllll(lep_pt,lep_eta,lep_phi,lep_E):\n",
    "    # first lepton is [0], 2nd lepton is [1] etc\n",
    "    px_0 = lep_pt[:,0]*np.cos(lep_phi[:,0]) # x-component of lep[0] momentum\n",
    "    py_0 = lep_pt[:,0]*np.sin(lep_phi[:,0]) # y-component of lep[0] momentum\n",
    "    pz_0 = lep_pt[:,0]*np.sinh(lep_eta[:,0]) # z-component of lep[0] momentum\n",
    "    px_1 = lep_pt[:,1]*np.cos(lep_phi[:,1]) # x-component of lep[1] momentum\n",
    "    py_1 = lep_pt[:,1]*np.sin(lep_phi[:,1]) # y-component of lep[1] momentum\n",
    "    pz_1 = lep_pt[:,1]*np.sinh(lep_eta[:,1]) # z-component of lep[1] momentum\n",
    "    px_2 = lep_pt[:,2]*np.cos(lep_phi[:,2]) # x-component of lep[2] momentum\n",
    "    py_2 = lep_pt[:,2]*np.sin(lep_phi[:,2]) # y-component of lep[2] momentum\n",
    "    pz_2 = lep_pt[:,2]*np.sinh(lep_eta[:,2]) # z-component of lep[3] momentum\n",
    "    px_3 = lep_pt[:,3]*np.cos(lep_phi[:,3]) # x-component of lep[3] momentum\n",
    "    py_3 = lep_pt[:,3]*np.sin(lep_phi[:,3]) # y-component of lep[3] momentum\n",
    "    pz_3 = lep_pt[:,3]*np.sinh(lep_eta[:,3]) # z-component of lep[3] momentum\n",
    "    sumpx = px_0 + px_1 + px_2 + px_3 # x-component of 4-lepton momentum\n",
    "    sumpy = py_0 + py_1 + py_2 + py_3 # y-component of 4-lepton momentum\n",
    "    sumpz = pz_0 + pz_1 + pz_2 + pz_3 # z-component of 4-lepton momentum\n",
    "    sumE = lep_E[:,0] + lep_E[:,1] + lep_E[:,2] + lep_E[:,3] # energy of 4-lepton system\n",
    "    return np.sqrt(sumE**2 - sumpx**2 - sumpy**2 - sumpz**2)/1000 #/1000 to go from MeV to GeV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1515ec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from methods from https://github.com/atlas-outreach-data-tools/notebooks-collection-opendata\n",
    "\n",
    "# cut on lepton charge\n",
    "# paper: \"selecting two pairs of isolated leptons, each of which is comprised of two leptons with the same flavour and opposite charge\"\n",
    "def cut_lep_charge(lep_charge):\n",
    "# throw away when sum of lepton charges is not equal to 0\n",
    "# first lepton is [0], 2nd lepton is [1] etc\n",
    "    return ((lep_charge[:,0] + lep_charge[:,1] + lep_charge[:,2] + lep_charge[:,3]) != 0)\n",
    "\n",
    "# cut on lepton type\n",
    "# paper: \"selecting two pairs of isolated leptons, each of which is comprised of two leptons with the same flavour and opposite charge\"\n",
    "def cut_lep_type(lep_type):\n",
    "# for an electron lep_type is 11\n",
    "# for a muon lep_type is 13\n",
    "# throw away when none of eeee, mumumumu, eemumu\n",
    "    sum_lep_type = lep_type[:,0] + lep_type[:,1] + lep_type[:,2] + lep_type[:,3]\n",
    "    return ((sum_lep_type != 44) & (sum_lep_type != 48) & (sum_lep_type != 52))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f451f832",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HZZAnalysis(Analysis):\n",
    "    def process(self, events):\n",
    "        # Get dataset name and lepton information from events\n",
    "        dataset = events.metadata['dataset']\n",
    "        leptons = events.lep\n",
    "        \n",
    "        # Cut events based on lepton charge, then update leptons\n",
    "        events = events[~cut_lep_charge(leptons.charge)]\n",
    "        leptons = events.lep\n",
    "        \n",
    "        # Cut events based on lepton type, then update leptons\n",
    "        events = events[~cut_lep_type(leptons.typeid)]\n",
    "        leptons = events.lep\n",
    "        \n",
    "        # Calculate the mllll for each event\n",
    "        mllll = calc_mllll(leptons.pt, leptons.eta, leptons.phi, leptons.energy)\n",
    "        \n",
    "        if (dataset == 'Data'):\n",
    "            # Create and fill a histogram for mllll\n",
    "            mllllhist = (\n",
    "                hist.Hist.new\n",
    "                .Reg(34, 80, 250, name='mllll', label='4-lepton invariant mass $\\mathrm{m_{4l}}$ [GeV]')\n",
    "                .StrCat(['Data'], name='dataset', label='Dataset')\n",
    "                .Weight()\n",
    "                .fill(mllll=mllll, dataset=dataset)\n",
    "            )\n",
    "        else:\n",
    "            # Extract the sample name from the filename metadata with regex\n",
    "            sample = re.findall(r'mc_\\d+\\.(.+)\\.4lep', events.metadata['filename'])[0]\n",
    "            \n",
    "            # Calculate the event weights\n",
    "            totalWeights = get_xsec_weight(sample) * events.mcWeight * events.scaleFactor\n",
    "            \n",
    "            # Create and fill a weighted histogram for mllll\n",
    "            mllllhist = (\n",
    "                hist.Hist.new\n",
    "                .Reg(34, 80, 250, name='mllll', label='4-lepton invariant mass $\\mathrm{m_{4l}}$ [GeV]')\n",
    "                .StrCat(['Background $Z,tt^{bar}$', 'Background $ZZ^{star}$', 'Signal ($m_H$ = 125 GeV)'], name='dataset', label='Dataset')\n",
    "                .Weight()\n",
    "                .fill(mllll=mllll, dataset=dataset, weight=totalWeights)\n",
    "            )\n",
    "        \n",
    "        return {\n",
    "            'entries': len(events),\n",
    "            'mllll': mllllhist\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31361726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from methods from https://github.com/gordonwatts/pyhep-2021-SX-OpenDataDemo\n",
    "\n",
    "def good_leptons(source: ObjectStream) -> ObjectStream:\n",
    "    '''Select out all good leptons from each event. Return their pt, eta, phi, and E, and other\n",
    "    things needed downstream.\n",
    "\n",
    "    Because uproot doesn't tie together the objects, we can't do any cuts at this point.\n",
    "    '''\n",
    "    return source.Select(lambda e:\n",
    "        {\n",
    "            'lep_pt': e.lep_pt,\n",
    "            'lep_eta': e.lep_eta,\n",
    "            'lep_phi': e.lep_phi,\n",
    "            'lep_energy': e.lep_E,\n",
    "            'lep_charge': e.lep_charge,\n",
    "            'lep_typeid': e.lep_type,\n",
    "            'mcWeight': e.mcWeight,\n",
    "            'scaleFactor': e.scaleFactor_ELE*e.scaleFactor_MUON*e.scaleFactor_LepTRIGGER*e.scaleFactor_PILEUP,\n",
    "        })\n",
    "\n",
    "def make_ds(name: str, query: ObjectStream):\n",
    "    '''Create a ServiceX Datasource for a particular ATLAS Open data file\n",
    "    '''\n",
    "    datasets = [ServiceXDataset(fileset[name], backend_name='uproot')]\n",
    "    return DataSource(query=query, metadata={'dataset': name}, datasets=datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ec9ce14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from method from https://github.com/gordonwatts/pyhep-2021-SX-OpenDataDemo\n",
    "\n",
    "async def run_analysis():\n",
    "    '''Run on a known analysis file/files and return the result.\n",
    "    Should be fine to start many of these at once.\n",
    "    '''\n",
    "    # Parse the dataset\n",
    "    ds_names = list(fileset.keys())\n",
    "\n",
    "    # Create the query\n",
    "    ds = ServiceXSourceUpROOT('cernopendata://dummy',  'mini', backend_name='uproot')\n",
    "    ds.return_qastle = True\n",
    "    leptons = good_leptons(ds)\n",
    "\n",
    "    # Get data source for this run\n",
    "    if os.environ[\"LABEXTENTION_FACTORY_MODULE\"] == \"coffea_casa\":\n",
    "    # We are using coffea-casa here, but we need to find beter handle to detect difference (TBD)\n",
    "        executor = DaskExecutor(client_addr=\"tls://localhost:8786\")\n",
    "    else:\n",
    "        executor = LocalExecutor()\n",
    "        \n",
    "    datasources = [make_ds(ds_name, leptons) for ds_name in ds_names]\n",
    "\n",
    "    # Create the analysis and we can run from there\n",
    "    analysis = HZZAnalysis()\n",
    "\n",
    "    async def run_updates_stream(accumulator_stream, name):\n",
    "        '''Run to get the last item in the stream'''\n",
    "        coffea_info = None\n",
    "        try:\n",
    "            async for coffea_info in accumulator_stream:\n",
    "                pass\n",
    "        except Exception as e:\n",
    "            raise Exception(f'Failure while processing {name}') from e\n",
    "        return coffea_info\n",
    "\n",
    "    all_plots = await asyncio.gather(*[run_updates_stream(executor.execute(analysis, source), source.metadata['dataset']) for source in datasources])\n",
    "\n",
    "    # Combine the MC plots\n",
    "    all_MC_plots = [p['mllll'] for p in all_plots[1:]]\n",
    "    MC_plot = all_MC_plots[0]\n",
    "    for p in all_MC_plots[1:]:\n",
    "        MC_plot += p\n",
    "\n",
    "    return {\n",
    "        'Data': all_plots[0]['mllll'],\n",
    "        'MC': MC_plot\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "180cbad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[root://eospublic.ce...:   0%|          | 0/9000000000.0 [00:00]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[root://eospublic.ce...:   0%|          | 0/9000000000.0 [00:00]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[root://eospublic.ce...:   0%|          | 0/9000000000.0 [00:00]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[root://eospublic.ce...:   0%|          | 0/9000000000.0 [00:00]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "Exception",
     "evalue": "Failure while processing Background $Z,tt^{bar}$",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7099/3909352362.py\u001b[0m in \u001b[0;36mrun_updates_stream\u001b[0;34m(accumulator_stream, name)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcoffea_info\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maccumulator_stream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/processor/servicex/executor.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, analysis, datasource, title)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mfinished_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstreamer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;32min\u001b[0m \u001b[0masync_accumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstreamer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/processor/accumulator.py\u001b[0m in \u001b[0;36masync_accumulate\u001b[0;34m(result_stream)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult_stream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/aiostream/stream/advanced.py\u001b[0m in \u001b[0;36mbase_combine\u001b[0;34m(source, switch, ordered, task_limit)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/aiostream/stream/create.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miscoroutinefunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/processor/servicex/executor.py\u001b[0m in \u001b[0;36minline_wait\u001b[0;34m(r)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;34m\"This could be inline, but python 3.6\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/distributed/client.py\u001b[0m in \u001b[0;36m_result\u001b[0;34m(self, raiseit)\u001b[0m\n\u001b[1;32m    244\u001b[0m                 \u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/processor/servicex/executor.py\u001b[0m in \u001b[0;36mrun_coffea_processor\u001b[0;34m()\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdata_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m         events = NanoEventsFactory.from_parquet(\n\u001b[0m\u001b[1;32m    160\u001b[0m             \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevents_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/coffea/nanoevents/factory.py\u001b[0m in \u001b[0;36mfrom_parquet\u001b[0;34m()\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0mfs_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfsspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             \u001b[0mtable_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParquetFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparquet_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParquetFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyarrow/parquet.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m()\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParquetReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         self.reader.open(source, use_memory_map=memory_map,\n\u001b[0m\u001b[1;32m    218\u001b[0m                          \u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyarrow/_parquet.pyx\u001b[0m in \u001b[0;36mpyarrow._parquet.ParquetReader.open\u001b[0;34m()\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0mget_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_memory_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m&\u001b[0m\u001b[0mrd_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mnogil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyarrow/io.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.get_reader\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1473\u001b[0;31m     \u001b[0mnf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_native_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_memory_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m     \u001b[0mreader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_random_access_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pyarrow/io.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.get_native_file\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1466\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert OpenFile to pyarrow.lib.NativeFile",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7099/1842051641.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mrun_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mfinish_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7099/3909352362.py\u001b[0m in \u001b[0;36mrun_analysis\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcoffea_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mall_plots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrun_updates_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalysis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dataset'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msource\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasources\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# Combine the MC plots\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_7099/3909352362.py\u001b[0m in \u001b[0;36mrun_updates_stream\u001b[0;34m(accumulator_stream, name)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Failure while processing {name}'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcoffea_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Failure while processing Background $Z,tt^{bar}$"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "output = await run_analysis()\n",
    "finish_time = time.time()\n",
    "\n",
    "print(\"Total runtime in seconds: \" + str(finish_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ca8527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the histograms\n",
    "hist.Hist.plot1d(output['Data'], histtype='errorbar', color='black')\n",
    "hist.Hist.plot1d(output['MC'], stack=True, histtype='fill', color=['purple', 'red', 'cyan'])\n",
    "\n",
    "# Basic bin parameters\n",
    "xmin = 80\n",
    "xmax = 250\n",
    "step_size = 5\n",
    "\n",
    "bin_centers = np.arange(start=xmin+step_size/2, # The interval includes this value\n",
    "                        stop=xmax+step_size/2, # The interval doesn't include this value\n",
    "                        step=step_size ) # Spacing between values\n",
    "\n",
    "# Calculate background statistical uncertainty\n",
    "# Remove the \"[:,:2]\" expressions to use all MC datasets with background and signal combined\n",
    "mc_tot_var = np.sum(output['MC'].variances()[:,:2], axis=1)\n",
    "mc_err = np.sqrt(mc_tot_var)\n",
    "mc_tot_height = np.sum(output['MC'].values()[:,:2], axis=1)\n",
    "\n",
    "# Plot background statistical uncertainty\n",
    "plt.bar(bin_centers, # x\n",
    "        2*mc_err, # heights\n",
    "        alpha=0.5, # half transparency\n",
    "        bottom=mc_tot_height-mc_err, color='none', \n",
    "        hatch=\"////\", width=step_size, label='Stat. Unc.')\n",
    "\n",
    "# Tune plot appearance\n",
    "main_axes = plt.gca()\n",
    "main_axes.set_xlim(left=xmin, right=xmax)\n",
    "main_axes.set_ylim(bottom=0, top=np.amax(output['Data'].values())*1.6)\n",
    "main_axes.set_ylabel('Events / '+str(step_size)+' GeV')\n",
    "main_axes.legend(frameon=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
